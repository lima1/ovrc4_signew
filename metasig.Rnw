\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]{hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{rotating}

\usepackage[parfill]{parskip}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
# this is equivalent to \SweaveOpts{...}
opts_chunk$set(fig.path='output/figures/metasig-', fig.align='center',
fig.show='hold',warning=FALSE, echo=FALSE)
options(replace.assign=TRUE,width=90)
@

\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{1}

\title{Risk Prediction for late-stage ovarian cancer by meta-analysis of 1,525
patient samples - Supplemental Information}

\author{Markus Riester, Wei Wei, Levi Waldron, Aedin Culhane, Lorenzo Trippa,
\\ Esther Olivia, Sung-hoon Kim, Franziska Michor, Curtis Huttenhower, \\
Giovanni Parmigiani, Michael Birrer}
\maketitle

Code and instructions to reproduce all results presented in this paper is
available at \url{https://bitbucket.org/lima1/ovrc4_signew}. 

\tableofcontents 

<<load, include=FALSE>>=
library(genefilter)
library(survival)
library(annotate)
library(hgu133a.db)
library(metafor)
library(ggplot2)
library(xtable)
library(ROCR)
library(RColorBrewer)
library(lattice)
library(maxstat)
library(exactRankTests)
library(pROC)
library(limma)
library(cvTools)
library(texreg)
library(GSVA)
library(ez)
library(sampling)
library(impute)
library(LeviRmisc)
library(RankProd)
library(gdata)

# only to print the version number in the Appendix. We load the data generated
# with the createEsetsList script
library(curatedOvarianData)

source("src/metaCMA.R")
source("src/utils.R")
source("src/ggplot2extras.R")
# fixes a bug in the survcomp package
source("src/forestplot.surv.R")
@

% load the filtered expression data (run ./runEsetList.sh in the input
% directory
<<cached_short_createesets, cache=TRUE>>=
load("input/eset.scaled.rda")
esets <- esets[-match("GSE19829.GPL8300_eset",names(esets))]
esets <- .fixTCGA(esets)
@


<<cached_short_filtergenes, cache=TRUE>>=
# use only genes common to all platforms
esets.f  <- metaCMA.common.gene.esets(esets)
@

<<removeyoshihara>>=
# we added the Yoshihara data after we finalized our model, so remove it for
# now and use it later for validation
esets.uf <- esets
japan.idx <- match("GSE32062.GPL6480_eset",names(esets.f))
esets.f <- esets.f[-japan.idx]
tcga_id <- match("TCGA_eset", names(esets.f))
@

<<cached_short_loadvaldata,cache=TRUE>>=
# Now load all the other data.
load("input/eset.binary.scaled.rda")
esets.binary <- esets
load("input/eset.validation.scaled.rda")
esets.validation_orig <- esets
load("input/eset.allos.scaled.rda")
esets.allos <- esets
load("input/eset.debulking.scaled.rda")
esets.debulking <- esets
@

<<changedebulkingtcga2>>=
esets.debulking <- .fixTCGA(esets.debulking)
esets.allos <- .fixTCGA(esets.allos)
@

<<addearlystage>>=
# add the TCGA early stage samples to the validation data
ids <- esets.allos$TCGA_eset$summarystage=="early" &
    esets.allos$TCGA_eset$summarygrade!="low"
esets.validation <- c( esets.validation_orig, TCGA_eset=esets.allos$TCGA_eset[, ids] )
@

<<loadtcga>>=
# Load the TCGA model from Levi's paper supplement
load("input/21720365-SuppTable_S6.RData")
model.tcga <- model.official
@

<<cached_short_loadjci, cache=TRUE>>=
# create the CLOVAR model from their supplement. They use the same TCGA
# algorithm
jci.sig <- read.xls("input/JCI65833sd1.xls", sheet=3, skip=1, as.is=TRUE)
jci.coefs <- ifelse(jci.sig$CLASS=="GOOD",-1,1)
names(jci.coefs) <- jci.sig$Gene.Symbol
model.jci <- new("linearriskscore", coefficients=jci.coefs, modeltype="tscore")
@

% calculate univariate Cox regression coefficients for each dataset and each
% gene
<<cached_medium_optimizecoefs, cache=TRUE>>=
coefs <- metaCMA.coefs(esets.f)
@

% Do our leave-one-dataset-out cross-validation for different gene-signature
% sizes. 
<<cached_medium_optimizegrid, cache=TRUE,results='hide'>>=
ma <- lapply(c(5,10,seq(25,250,25)), function(i)
    #metaCMA(esets.f,coefs=coefs,n=i,method="penalizedSurv",fold=5))
    metaCMA(esets.f,coefs=coefs,n=i))
@

<<cached_medium_reml, cache=TRUE,results='hide'>>=
# random-effects model
reml <- metaCMA(esets.f,coefs=coefs,n=200, rma.method="REML")
reml.opt <- metaCMA.opt(esets.f,coefs=coefs,n=200, rma.method="REML")
@

\clearpage
\normalsize

\section{Leave-one-dataset-out cross-validation}

Instead of arbitrarily setting aside some large and informative datasets for
validation, we initially tested our models in a leave-one-dataset-out
cross-validation procedure (Supplemental Figure~\ref{fig:flowchart}). The goal
of this procedure is to build the best possible model by using all major
datasets for training, and to still obtain an accurate (in particular not
over-optimistic) estimate of how well this model will perform when applied to
independent data. 

For the overall survival signature, we first selected the 6 largest datasets
as training datasets. We excluded the small datasets ($< 75$)
because small datasets often display unusual patient characteristics
and because they have minor impact on the feature selection and are thus more
valuable as test than as training datasets. Then we trained 6 different models
and always excluded one dataset from training.  These 6 models were then
validated each time on the corresponding excluded datasets. This procedure thus
results in one risk prediction per patient, which can then be used for model
evaluation by comparing the risk prediction with the observed outcome. This is
not an over-optimistic evaluation, because to calculate a particular patient's
risk score, only information from different patient cohorts was utilized. 

\begin{figure}[!htb]
{\centering \includegraphics[width=0.5\textwidth]{input/figures/flowchartLODO} 

}
\caption{Flowchart leave-one-dataset-out cross-validation algorithm.}
\label{fig:flowchart}
\end{figure}


% This now uses all datasets for calculating the final model.
<<cached_short_finalmodel,cache=TRUE>>=
tmp <- metaCMA.opt(esets.f,coef=coefs,n=200)
final.model <- tmp$model
@

<<cached_short_finalmodelall, cache=TRUE>>=
tmpall <- metaCMA.opt(esets.f,coef=coefs,n=nrow(esets.f[[1]]))
@

<<tablemodel>>=
# Print the model for the supplement
sig <- names(final.model@coefficients)
esets.f.all <- esets.f
esets.f.all$GSE32062.GPL6480_eset <- esets.validation$GSE32062.GPL6480_eset

probesets <- do.call(cbind, lapply(esets.f.all, function(X) {
 featureNames(X) <- make.names(featureNames(X))
 as.character(featureData(X)[sig,]$maxmean_probeset)
 }))

tmp.df <- data.frame(Genes=sig, probesets, "Pooled Cox Coefficient"=
 final.model@coefficients,
 P=head(sort(tmp$pvalues),length(sig)),
 FDR=head(sort(p.adjust(tmp$pvalues,method="BH")),length(sig))
)
write.csv(tmp.df, file="output/final_signature.csv", row.names=FALSE)
write.table(tmp.df, file="output/final_signature.tsv", row.names=FALSE,
sep="\t", quote=FALSE)
@

<<tcgasubtypes>>=
# get the official TCGA subtype labels and add it to the eset
# this was obtained from a Broad mailing list, because the supplement refers
# to an internal link
subtypes <- read.delim("input/TCGA_489_UE.k4.txt")

esets.f$TCGA_eset$tcga_subtype_official <-
    subtypes[match(make.names(substr(esets.f$TCGA_eset$alt_sample_name,1,20)),
    subtypes[,1]),2]
@

<<cached_short_learnsubtypegenesets, cache=TRUE>>=
# here we use Limma to find gene sets for the TCGA subtype 
# we use again a default signature size of 200 (100 up and 100 down-regulated
# genes. The fold-change was chosen so that ~100 genes passed this filter with
# a FDR cutoff of 0.001.
X <- esets.f$TCGA_eset[, !is.na(esets.f$TCGA_eset$tcga_subtype_official)]
genesets <- .getFingerprintGeneSets(X, as.factor(X$tcga_subtype_official), p.value=0.001,
lfc=log2(1.45), number=100)
@

<<cached_short_subtypes_classifyesets,cache=TRUE,results='hide'>>=
gsva.res <- lapply(esets.f, function(X) .classifyGSVA(eset=X, genesets=genesets))
gsva.res.val <- lapply(esets.validation, function(X) .classifyGSVA(eset=X, genesets=genesets))
@

<<addsubtyping>>=
# get the subtype labels out of the GSVA results 
.fetchSubtypeGSVA <- function(i, esets, gsva, na.cutoff=0.1,
labels=levels(esets.f$TCGA_eset$tcga_subtype_official) ) {
    tcga_subtype <- apply(gsva[[i]],2,which.max)
    # perfect enrichment score would be 2.0 and here we filter a few samples which
    # we really can't classify. This is still not very conservative, but does not
    # change much when I increase this threshold.
    tcga_subtype[apply(gsva[[i]],2,max) < na.cutoff] <- NA
    as.factor( labels[as.numeric(tcga_subtype)] )
}
for (i in 1:length(esets.f)) esets.f[[i]]$tcga_subtype <- .fetchSubtypeGSVA(i, esets.f, gsva.res)
for (i in 1:length(esets.validation)) {
    esets.validation[[i]]$tcga_subtype <-
        .fetchSubtypeGSVA(i, esets.validation, gsva.res.val)
}        

@

<<cached_short_prepareclinicaldf, cache=TRUE>>=
# Create a data.frame that contains all relevant clinical information and
# the TCGA and our predictions. Use esets.uf, because esets.f contains only
# probesets common to all platforms.
X <- esets.uf[-japan.idx]
risk_tcga <- lapply(X, function(X) predict(model.tcga, newdata=X, type="lp")@lp)
risk_jci <- lapply(X, function(X) predict(model.jci, newdata=X, type="lp")@lp)
X <- esets.f
for (i in 1:length(X)) {
    X[[i]]$risk_tcga <- risk_tcga[[i]]
    X[[i]]$risk_jci  <- risk_jci[[i]]
    X[[i]]$risk <- ma[[10]]$fits[[i]]$risk@lp
    X[[i]]$risk_reml <- reml$fits[[i]]$risk@lp
}    
X <- X[-grep("TCGA",names(X))]
dfrc <- .createClinical(X)
dfrc$risk_tcga <- unlist(sapply(X, function(x) x$risk_tcga))
dfrc$risk_jci <- unlist(sapply(X, function(x) x$risk_jci))
dfrc$risk <- unlist(sapply(X, function(x) x$risk))  
dfrc$risk_reml <- unlist(sapply(X, function(x) x$risk_reml))

idx.stagedeb <- !is.na(dfrc$age) & !is.na(dfrc$debulking)
idx.stagedeb <- !is.na(dfrc$tumorstage) & !is.na(dfrc$debulking)
dfrc$debulking_orig <- dfrc$debulking
dfrc$debulking <- as.numeric(dfrc$debulking)
@

<<cached_short_clinicalvalidation, cache=TRUE>>=
# make another data.frame with clinical information as signature risk scores,
# this time for the validation data
X <- esets.validation
dfrcval <- .createClinical(esets.validation)
for (i in 1:length(X)) X[[i]]$risk <- predict(model.tcga, newdata=X[[i]], type="lp")@lp
dfrcval$risk_tcga <- unlist(sapply(X, function(x) x$risk))
for (i in 1:length(X)) X[[i]]$risk <- predict(model.jci, newdata=X[[i]], type="lp")@lp
dfrcval$risk_jci <- unlist(sapply(X, function(x) x$risk))
for (i in 1:length(X)) X[[i]]$risk <- predict(final.model, newdata=X[[i]], type="lp")@lp
dfrcval$risk <- unlist(sapply(X, function(x) x$risk))
for (i in 1:length(X)) X[[i]]$risk <- predict(reml.opt$model, newdata=X[[i]], type="lp")@lp
dfrcval$risk_reml <- unlist(sapply(X, function(x) x$risk))

dfrcval$debulking_orig <- dfrcval$debulking
@

<<dfrcboth>>=
# combine both leave-one-dataset-out validation and independent validation in
# one easy to work with data.frame
dfrcboth <- rbind(dfrc, dfrcval)
dfrcboth <- dfrcboth[!grepl("GenomeAtlas",dfrcboth$batch ),]

# thanks Hmisc for messing up the Surv function
dfrcboth$y <- Surv(dfrcboth$y[,1], dfrcboth$y[,2])

# add the ssGSEA subtype scores as covariates to the data.frame
g <- do.call(cbind, lapply(1:4, function(st)
 unlist(lapply(c(gsva.res[-tcga_id],gsva.res.val[-4]), function(x) x[st,]))))
colnames(g) <-  rownames(gsva.res[[1]])
dfrcboth <- data.frame(dfrcboth, g)
@

<<cached_short_figure1,cache=TRUE,fig.width=7,fig.height=4.7,out.width="0.9\\textwidth",out.height="0.6\\textwidth",results='hide',include=FALSE>>=
res.lodo <- .lodocvPlot(esets.f, models=lapply(ma[[10]]$fits, function(x)
    x$fit),ids=1:tcga_id)[-tcga_id]
res.lodo.tcga <- .lodocvPlot(esets.f,
plot=FALSE,models=model.tcga,ids=1:(tcga_id-1))
res.lodo.jci <- .lodocvPlot(esets.f,
plot=FALSE,models=model.jci,ids=1:(tcga_id-1))
@

<<cached_short_validationexternal,cache=TRUE,results='hide', include=FALSE>>=
res.val <- .valPlot(final.model)
res.val.tcga <- .valPlot(model.tcga, plot=FALSE)
res.val.jci <- .valPlot(model.jci, plot=FALSE)
@

<<addtodfrcboth>>=
# here we add the patient stratifications (high-risk vs low-risk) to the data.frame
dfrcboth$strata <-  unlist(lapply(c(res.lodo,res.val[-4]), function(x)
    x$strata))

dfrcboth$strata_tcga <-  unlist(lapply(c(res.lodo.tcga,res.val.tcga[-4]), function(x)
    x$strata))

dfrcboth$strata_jci <-  unlist(lapply(c(res.lodo.jci,res.val.jci[-4]), function(x)
    x$strata))
@

\clearpage
\section{Fixed- vs. random-effects survival signature}
\label{sec:fixed}
In this section, we explore how two different options for pooling the
regression coefficients across datasets affect the final overall survival
model. We compared a fixed-effects meta-analysis, in which coefficients
are weighted by the inverse of their squared standard errors, with a
random-effects model, which uses the
restricted maximum-likelihood method, the default method in the
\texttt{metafor} R package \cite{Viechtbauer:2005}.

<<cached_medium_fevsre, cached=TRUE>>=
fe.model <- final.model
re.model <- reml.opt$model
a <- metaCMA.compare(fe.model, re.model, esets.f, coefs=coefs)
@

<<fevsre_cnt>>=
hetgenes <- sapply(a$genes, function(g) a$fe$rma[[g]]$QEp)
hetgenes <- names(hetgenes[hetgenes < 0.05])
nhet <- length(hetgenes) 
nint <- length(intersect(names(fe.model@coefficients),
    names(re.model@coefficients)))
nunion <- length(union(names(fe.model@coefficients),
    names(re.model@coefficients)))
@

The overlap of the gene signatures obtained with a fixed- and a random-effects
gene ranking is shown in Supplemental Figure~\ref{fig:effects:venn}. Out of
the 200 genes in each signature, \Sexpr{nint} were present in both signatures.
The choice of the meta-analysis method had no marked impact on the pooled
Cox regression coefficients; the \Sexpr{nunion} genes utilized in the two
signatures had highly similar coefficients across the fixed- and random-effects
meta-analysis (Supplemental Figure~\ref{fig:effects:coef}).  Supplemental
Figure~\ref{fig:effects:hm1} to \ref{fig:effects:hm3} show the Cox regression
coefficients in all datasets, focussing on the genes present in only the fixed-effects
signature, only the random-effects signature and present in both signatures,
respectively (these cases correspond to the 3 sections of the Venn diagram). Genes with
relatively higher heterogeneity across studies are marked in black in the heatmap.  In the
fixed-effects signature, \Sexpr{nhet} genes displayed statistically significant
heterogeneity ($P < 0.05$, Q-Test \cite{Hedges:1985}).  When accounting for
heterogeneity, probesets with heterogeneity drop in their ranks and are
replaced in the random-effects signature with probesets displaying less
heterogeneity (Supplemental Figure~\ref{fig:effects:hm2}). 

<<intcor, cache=TRUE, results='hide'>>=
idx <- !sapply(esets.f, .defaultFilter)
xx1 <- esets.f[idx][[1]]
xx3 <- esets.f[idx][[3]]
xx4 <- esets.f[idx][[4]]
xx6 <- esets.f[idx][[6]]
xxx <- mergeExprs(xx1,xx3,xx4,xx6)
ic1b <- intcor(xxx[c(1,3)])
ic2b <- intcor(xxx[c(2,3)])
ic1t <- intcor(xxx[c(1,4)])
ic2t <- intcor(xxx[c(2,4)])
@

<<intocoravg>>=
ic1 <- apply(cbind(ic1b[[1]],ic1t[[1]]),1,mean)
ic2 <- apply(cbind(ic2b[[1]],ic2t[[1]]),1,mean)
@

\begin{figure}
\centering
\subfigure[Venn diagram signature overlaps]{
\label{fig:effects:venn}
<<fevsre_plotting_venn, out.width="0.38\\textwidth">>=
venn(list("Random-Effects"=names(re.model@coefficients), 
     "Fixed-Effects"=names(fe.model@coefficients)),small=1.5)
@
}
\subfigure[Comparison of pooled regression coefficients]{
\label{fig:effects:coef}
<<fevsre_plotting_corcoef, out.width="0.38\\textwidth">>=
 plot(sapply(a$fe$rma[a$genes], function(x) x$b), sapply(a$re$rma[a$genes],
 function(x) x$b), xlab="Pooled Cox regression coefficients (fixed-effects)",
 ylab="Pooled Cox regression coefficients (random-effects)", las=1,
 cex.axis=1.4, cex.lab=1.45 )
@
}\\
\subfigure[Genes only in FE signature]{
\label{fig:effects:hm1}
<<fevsre_plotting_1, out.width="0.38\\textwidth">>=
difffe <- setdiff(names(fe.model@coefficients), names(re.model@coefficients))
difffe <- difffe[ difffe %in% rownames(coefs[[1]])]

cols <- ifelse(difffe %in% a$genes[sapply(a$genes, function(g) a$fe$rma[[g]]$QEp) < 0.05], "black", "white")
x <- coefs[[1]][difffe,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}
\subfigure[Genes only in RE signature]{
\label{fig:effects:hm2}
<<fevsre_plotting_2, out.width="0.38\\textwidth">>=
diffre <- setdiff(names(re.model@coefficients), names(fe.model@coefficients))
diffre <- diffre[ diffre %in% rownames(coefs[[1]])]

cols <- ifelse(diffre %in% a$genes[sapply(a$genes, function(g) a$re$rma[[g]]$QEp) < 0.05], "black", "white")
x <- coefs[[1]][diffre,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}\\
\subfigure[Genes in both FE and RE signature]{
\label{fig:effects:hm3}
<<fevsre_plotting_3, out.width="0.38\\textwidth">>=
intersectrefe <- intersect(names(re.model@coefficients), names(fe.model@coefficients))
intersectrefe <- intersectrefe[ intersectrefe %in% rownames(coefs[[1]])]

cols <- ifelse(intersectrefe %in% a$genes[sapply(a$genes, function(g) a$re$rma[[g]]$QEp) < 0.05], "black", "white")
x <- coefs[[1]][intersectrefe,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}
\subfigure[Integrative Correlation Analysis]{
\label{fig:effects:intcor}
<<intcorplot, out.width="0.38\\textwidth">>=
pdiff <- round(t.test(c( ic1[diffre],  ic2[diffre])  ,c( ic1[hetgenes]
    ,ic2[hetgenes]  ))$p.value, digits=2)

d.f <- rbind( data.frame(IntCor=ic1[diffre], Dataset=.getDatasetNames(esets.f)[1],
Method="RE"),
data.frame(IntCor=ic1[hetgenes], Dataset=.getDatasetNames(esets.f)[1],
Method="FE"),
data.frame(IntCor=ic2[diffre], Dataset=.getDatasetNames(esets.f)[3],
Method="RE"),
data.frame(IntCor=ic2[hetgenes], Dataset=.getDatasetNames(esets.f)[3],
Method="FE"))
d.f$Method <- factor(d.f$Method, levels=c("FE", "RE"))
ggplot(d.f,aes(Method,IntCor,fill=Dataset))+geom_boxplot()+
ylab("Integrative Correlation")+theme_classic2(15)
@
}
\caption{Comparison of a fixed- (FE) and a random-effects (RE) ranking of genes. See
main text for a description of the panels.}
\label{fig:effects}
\end{figure}

The observed heteogeneity is almost exclusively caused by differences in the
effects estimates from the Yoshihara 2010 and the Bentink et al. datasets. As
a result, the random-effects analysis removed genes that have strong effect in
most training datasets, but no effect or a discordant effect in either of
these two studies. The random-effects model then replaced these genes with
ones that have slightly weaker, but more consistent effect. This can be an
advantage or a disadvantage depending on whether the heterogeneity
is indicative of biological variation or of technology driven study-specific
artifact.

To explore whether technical issues in these studies are the main
sources of heterogeneity, we used Integrative Correlation Analysis
\cite{Parmigiani:2004}.  A low integrative correlation of a gene
indicates unusual pattern of expression correlations when compared to two reference datasets (Bonome
and TCGA). We reported averaged results using once the Bonome
and once the TCGA dataset as reference. 

The analysis was done utilizing the \texttt{MergeMaid} \cite{manZhong} and
\texttt{metaArray} \cite{manGosh} R packages with default Pearson
Correlation. We then tested whether the genes with heterogeneity have lower
integrative correlation, indicative of unusual expression, compared to the genes
exclusively found in the random-effects signature (Supplemental
Figure~\ref{fig:effects:intcor}). The analysis was performed independently for
both the Yoshihara 2010 and the Bentink datasets. In both datasets,
genes with heterogeneity tend to have lower integrative correlation than genes
without, indicating that technical issues are at least partly a source of
heterogeneity. The difference was however not statistically significant ($P =
\Sexpr{pdiff}$, two-sided Student t-test).

\clearpage

\section{Survival gene signature size}

In all our analyses, we fixed the gene signature size to 200 genes. This size
was motivated by the fact that this size is sufficiently small to be
practically useful in a clinical test and by the performance of validated and
random signatures \cite{Waldron:2014}. It was shown in Waldron et al.  that
smaller signatures tend to be less robust than large signatures.  Furthermore,
larger signatures allow the use of pathway enrichment tools, which are often
useful for prioritizing genes for further experimental validation.  Finally,
because we plan to apply the models to a large cohort of FFPE specimens, we
decided for a larger signature to provide some redundancy.  This is important
as we expect that some genes in the signatures will not be available after
quality controlling.

Our algorithm weighs genes according their rank (see Methods).  This means
increasing the signature size is expected to have only limited influence on
the prediction performance at some point as the weights of the genes decrease.
Increasing the number of features in an ensemble classifier such as the
compound covariate score is not prone to the overfitting as in multivariable
regression, where one may optimize a model to perfectly explain the data.
All tuning parameters - algorithmic details, training datasets and gene
signature size - were determined beforehand to allow an unbiased use of the
training data for testing purposes in our leave-one-dataset-out testing
procedure.

In Supplemental Figure~\ref{fig:cutoff:train} we confirm that the signature
size had only modest impact on the prediction accuracy in our algorithm, as
long as the signature size was larger than 100 genes.  In this figure, the
prediction accuracy is reported with the C-Index metric. The C-Index is a
pairwise comparison of patients, summarizing the fraction of pairs where the
patient predicted to be at higher risk in fact has shorter survival.  A
C-Index of 0.5 would correspond to a random model, and a C-Index of 1.0 of to
a perfect model. Such a perfect model would predict the correct order in which
patients die. We chose this performance measure here instead of Hazard Ratios
because it has an easy interpretation, is essentially parameter free and does
not require a dichotomization of the prediction scores.

\begin{figure}[!htb]
<<cached_short_validate, out.width='0.75\\textwidth', cache=TRUE>>=
yi <- lapply(ma, function(x) sapply(1:length(esets.f), function(i)
    evaluate(x$fits[[i]]$risk, measure=new("UnoC"),
    newy=esets.f[[i]]$y,add=list(tau=365.25*4) )))

facetAdjust(.plotN(esets.f, yi))
@
\caption{We used for all our gene signatures a fixed gene signature size of
200 genes. Here we show the influence of this cutoff on the prediction
concordance of the overall survival signature. Each point represents the
prediction concordance of a model with $x$ genes in the corresponding dataset
that was trained using the remaining datasets only.}
\label{fig:cutoff:train}
\end{figure}

\section{Comparison with the TCGA signatures}

We compared our signature to the TCGA signature \cite{TCGA:2011}. To apply the
TCGA signature across microarray platforms, we matched the 193 probe sets in
the signature to 185 unique gene symbols \cite{Seal:2011} used in
curatedOvarianData. We first reproduced the reported performance of this model
in the three TCGA test datasets \cite{Bonome:2008,Dressman:2007,Tothill:2008}
to ensure the correctness of our model implementation (Supplemental
Figure~\ref{fig:tcga:reproduce}). Among the 200 genes in our signature, 17
overlapped with the TCGA signature ($P < 0.001$).  The p-value of this overlap
between our and the TCGA signature was calculated with the hypergeometric
distribution, using the number of genes common to all training datasets as
background. 

<<cached_medium_cdiffconfint, cache=TRUE>>=
cci <- Inf.Cval.Delta(as.matrix(dfrcboth$y), dfrcboth$risk,
    dfrcboth$risk_tcga, tau=365.25*4)  
@

<<cdiff>>=
ccir <- round(cci, digits=3)
@

We used standard bootstrap to assess the statistical significance of the
difference in Hazard Ratios (HR) between our signature and TCGA's. We
considered all cohorts together, excluding TCGA, resulting in a total of
\Sexpr{nrow(dfrcboth)} patients where direct comparison to the TCGA signature
could be made. We generated 10,000 bootstrap replicates using standard
sampling with replacement from this pool of \Sexpr{nrow(dfrcboth)}  patients.
From these replicates we estimated confidence intervals for the individual HRs
and their difference. We reported p-value corresponding to the fraction of
bootstrap replicates with higher HR in TCGA.  Note that the confidence
interval of the difference in HR may exclude zero (thus resulting in a
statistically significant difference) even though the two confidence intervals
of the individual signatures are overlapping. We further calculated the
improvement in C-Index of our model compared to TCGA and found only a moderately,
not statistically significant improvement from \Sexpr{ccir[1,1]} for TCGA to
\Sexpr{ccir[2,1]} for our model. 

<<cached_medium_hrvscindex,cache=TRUE, results='hide', fig.keep='none'>>=
.eval <- function(X) {
    cindex <- Est.Cval(cbind(X$y, X$risk), tau=365.25*4)$Dhat
    logtest <-  summary(coxph(y~risk,X))$logtest[1]
    c(cindex, logtest)
}      

.swap <- function(X) {
    x <- sample(nrow(X),1)
    X[x,]$risk <- sort(dfrcboth$risk)[sample(nrow(X),1)]
    X
}

.hr <- function(X) {
as.numeric(gsub("^HR |;.*$","", plotKMStratifyBy("median", y=X$y,X$risk,
censor.at=365.25*5)$hr))
}

.optimize <- function(X, n, max.cindex=0.64,oi="", verbose=TRUE) {
best <- .eval(X)
for (i in 1:n) {
    
    Xnew <- .swap(X)
    res <- .eval(Xnew)
    if (verbose) cat(oi, max.cindex, res,"\n")
    if (res[2] > best[2]) { 
        X <- Xnew
        best <- res
    }    
    if (res[1] >= max.cindex) break 
}
.hr(X) 
}

set.seed(1234)
res.opt <- sapply(1:25, function(i) .optimize(dfrcboth, 10000, max.cindex=cci[2,1]+cci[3,4], oi=i))
res.opt <- res.opt - .hr(dfrcboth)
mres <- round(mean(res.opt), digits=2)
@

Since the C-Index is widely known to be relatively insensitive to prediction
improvements \cite{Pepe:2013}, we tried to estimate the required improvement
in Hazard Ratio to achieve significance on the C-Index scale. To this end, we
simulated better versions of our model with statistically significantly
improved C-Index. We utilized a greedy Monte Carlo optimization (start with
our risk scores; randomly select a patient; sample a new risk score for this
patient from our risk score distribution; accept the change if the Cox model
is improved; stop when the C-Index improvement over TCGA achieves
significance; repeat this simulation 25 times). We observe that in these
simulations, an additional HR improvement of \Sexpr{mres} over the 2.19 of our
model (to 2.46, compared to 1.83 for TCGA, see Figure 4 in the main paper)
would be needed to achieve a statistically significant improvement in C-Index.
Note this simulation assumes independence of risk scores (patients with similar
expression profile would have similar risk scores, however), but the only
purpose of this simulation is to illustrate the relationship of the scales of
HR and C-Index.

The TCGA project recently published an improved gene signature called CLOVAR
\cite{Verhaak:2013}. We could map 87 of the 100 genes of the signature to
probesets used in curatedOvarianData. 21 of these genes overlapped with our
gene signature ($P < 0.001$). 

% Validate the TCGA model. Stolen from Levi's paper supplement.
<<cached_short_validatetcgamodel, cache=TRUE,results='hide'>>=
data(TCGA_eset, package="curatedOvarianData")
featureNames(TCGA_eset) <- sub("-", "hyphen", featureNames(TCGA_eset))
TCGA_eset <- TCGA_eset[ ,!is.na(TCGA_eset$days_to_death) &
                                             !is.na(TCGA_eset$vital_status)]
TCGA_eset$y <- Surv(TCGA_eset$days_to_death / 30,
                               TCGA_eset$vital_status == "deceased")

TCGA.validation.eset <- TCGA_eset[ ,TCGA_eset$batch %in% c("17", "18", "19", "21", "22", "24")]
TCGA.training.eset   <- TCGA_eset[ ,TCGA_eset$batch >= 9 & TCGA_eset$batch <=
15 & !is.na(TCGA_eset$batch)]
##Validation set 2:
data(GSE9891_eset, package="curatedOvarianData")
featureNames(GSE9891_eset) <- sub("-", "hyphen", featureNames(GSE9891_eset))
GSE9891_eset <- GSE9891_eset[ ,!is.na(GSE9891_eset$days_to_death) &
                             !is.na(GSE9891_eset$vital_status)]
GSE9891_eset$y <- Surv(GSE9891_eset$days_to_death / 30, GSE9891_eset$vital_status == "deceased")

#Validation set 3, Bonome et al. (2008):
data(GSE26712_eset, package="curatedOvarianData")
featureNames(GSE26712_eset) <- sub("-", "hyphen", featureNames(GSE26712_eset))
GSE26712_eset <- GSE26712_eset[ ,!is.na(GSE26712_eset$days_to_death) &
                               !is.na(GSE26712_eset$vital_status)]
GSE26712_eset$y <- Surv(GSE26712_eset$days_to_death / 30,
                        GSE26712_eset$vital_status == "deceased")
data(PMID17290060_eset, package="curatedOvarianData")
featureNames(PMID17290060_eset) <- sub("-", "hyphen", featureNames(PMID17290060_eset))
PMID17290060_eset <- PMID17290060_eset[ ,!is.na(PMID17290060_eset$days_to_death) &
                                       !is.na(PMID17290060_eset$vital_status)]
PMID17290060_eset$y <- Surv(PMID17290060_eset$days_to_death / 30,
                            PMID17290060_eset$vital_status == "deceased")
@

\begin{figure}
<<plotmodeltcgavalidation, results='hide',fig.width=10,fig.height=10,out.width="0.96\\textwidth",out.height="0.96\\textwidth">>=
par(mar=c(5,5,3,2), mfrow=c(2,2))

plot(model.tcga,newdata=TCGA.validation.eset,newy=TCGA.validation.eset$y,censor.at=60,
            show.legend=FALSE,xlab="",ylab="Overall Survival (%)",main="TCGA test set")

plot(model.tcga,newdata=GSE9891_eset,newy=GSE9891_eset$y,censor.at=60,
            show.legend=FALSE,xlab="",ylab="",main="Tothill")
            
plot(model.tcga,newdata=GSE26712_eset,newy=GSE26712_eset$y,censor.at=60,
            show.legend=FALSE,xlab="Time (Months)",ylab="Overall Survival (%)",main="Bonome")

plot(model.tcga,newdata=PMID17290060_eset,newy=PMID17290060_eset$y,censor.at=60,
            show.legend=FALSE,xlab="Time (Months)",ylab="",main="Dressman")
@
\caption{TCGA model applied to the author test sets shown in Figure 2c of the TCGA paper
\cite{TCGA:2011}. The identical results show that our implementation of the
TCGA model is correct. Red survival curves correspond to high risk patients,
blue curves to low risk patients.}
\label{fig:tcga:reproduce}
\end{figure}

\clearpage

\section{Comparison with the CLOVAR multivariable model}

<<accuracytcga>>=
x <- table(esets.f$TCGA_eset$tcga_subtype_official, esets.f$TCGA_eset$tcga_subtype)
x <- round(sum(sapply(1:4, function(i) x[i,i]))/sum(!is.na(esets.f$TCGA_eset$tcga_subtype_official))*100,digits=1)
@

We classified all datasets by TCGA subtype using a single sample GSEA variant
as implemented in the GSVA package. Subtype specific gene sets were first
identified with the limma package in the TCGA data using the official TCGA
subtype labels. Final subtype scores were obtained by subtracting the ssGSEA
scores for the down-regulated gene sets from the up-regulated genes. We again
used default gene set sizes of 200 genes for each subtype (100 up- and 100
down-regulated genes per subtype). Applied back to the TCGA data, this
approach classified \Sexpr{x}\% of the samples correctly. Note that TCGA used
unified expression measures obtained from multiple platforms for training, not
the Affymetrix data we used in our meta-analysis. In Supplemental
Figure~\ref{fig:km:subtypesos}, we show an association of subtype with overall
survival in all datasets except TCGA consistent with the report of Verhaak et
al. \cite{Verhaak:2013}. The immunoreactive subtype had in both TCGA and the
remaining datasets the best prognosis. Poor survival was in general observed
for samples classified as mesenchymal. 

<<aocscomparison>>=
clinical.aocs <- read.xls("input/AOCS_subtytpes.xlsx", as.is=TRUE)
clinical.aocs$X <- toupper(gsub(".cel$","", clinical.aocs$X))
esets.f$GSE9891_eset$aocs_subtype <-
    as.factor(clinical.aocs$k[match(sampleNames(esets.f$GSE9891_eset),
    clinical.aocs$X)])

tbl.aocs <- table( esets.f$GSE9891_eset$aocs_subtype, 
    apply(gsva.res$GSE9891_eset,2,which.max))
colnames(tbl.aocs) <- rownames(gsva.res[[1]])
@

\begin{figure}
<<subtypesosdiff,fig.width=10,fig.height=10,out.width="0.96\\textwidth",out.height="0.96\\textwidth">>=
par(mar=c(5,5,3,2), mfrow=c(2,2))
plotKM(y=dfrcboth$y, strata=dfrcboth$tcga_subtype,
censor.at=365.25*5,main="A)  All datasets except TCGA")
plotKM(y=esets.f$TCGA_eset$y, strata=esets.f$TCGA_eset$tcga_subtype,
censor.at=365.25*5,main="B)  TCGA")
plotKM(y=esets.f$GSE9891_eset$y, strata=esets.f$GSE9891_eset$aocs_subtype,
censor.at=365.25*5,main="C)  AOCS Subtypes in Tothill et al.")
@
\caption{Association of subtype and overall survival.  (A) All training and
validation datasets excluding TCGA. (B) Stratification of TCGA samples by
subtype. (C) Kaplan-Meier curves of the subtypes proposed by the Australian
Ovarian Cancer Study Group (AOCS) in Tothill et al \cite{Tothill:2008}. This
analysis corresponds to Figure 5B of the Tothill study, with the difference
that here we show only the late-stage, high-grade, serous tumors used in our
meta-analysis.}
\label{fig:km:subtypesos}
\end{figure}

We then tested for consistent overlaps with the Australian Ovarian Cancer
Study Group (AOCS) subtypes as published by Tothill et al.
\cite{Tothill:2008}. All mesenchymal samples were assigned to the AOCS cluster
c1, which had poor prognosis in the Tothill data (Supplemental
Figures~\ref{fig:km:subtypesos}C and \ref{fig:overlap:aocstcga}). Most
immunoreactive samples were assigned to the c2 cluster and c2 samples had
consistent with our results better outcome in the Tothill dataset.


\begin{figure}
<<subtypesaocsoverlap,fig.width=10,fig.height=10,out.width="0.96\\textwidth",out.height="0.96\\textwidth">>=
heatmap.2(tbl.aocs,col=topo.colors,trace="none",margin=c(12,5))
@
\caption{Comparison AOCS and TCGA subtypes in Tothill et al.
\cite{Tothill:2008}. Here we compare our classification of Tothill samples in
TCGA subtypes with the official Tothill et al. clusters. The colors as
indicated in the legend on the upper left corner visualize the number of
patients in the pairwise AOCS (rows) and TCGA (columns) subtype combinations.
For example, 35 patients of the AOCS cluster c1 were classified as
Mesenchymal.}
\label{fig:overlap:aocstcga}
\end{figure}


Verhaak et al. proposed a multivariable model including tumor stage,
debulking status, BRCA1/2 mutation status and ssGSEA scores for the
immunoreactive and mesenchymal subtypes.  BRCA1/2 mutation status was
unavailable for our validation datasets. As the survival association of the
ssGSEA scores were discovered in most of our validation datasets, we tested a
multivariable model using the CLOVAR patient stratifications in high- and
low-risk (based on the median of CLOVAR risk scores in all datasets except the
validation data), tumor stage, debulking status and ssGSEA scores for all 4
subtypes. This model was then 5-fold cross-validated using all datasets
combined. This model performed very similarily to the one proposed by the
authors using only 2 ssGSEA scores, but did not require any biased feature
selection.

The Pearson correlation of the risk scores of our meta-analysis signature and both
TCGA signatures (for Verhaak et al. based on the CLOVAR signature only, not
the multivariable predictions) are shown in Supplemental Figure~\ref{fig:cor:risk}. 
In this figure, we further compare the models with the random-effects variant of
our model, in which the univariate Cox regression coefficients were pooled
with a random-effects model as implemented in the \texttt{metafor} package.
The predictions from the random-effects model were very similar compared to
the default fixed-effects model (Pearson correlation > 0.99).

% Prepare the debulking datasets (use only genes common to all platforms)
<<debulkingmetaprepare>>=
esets.debulking.f <- metaCMA.common.gene.esets(esets.debulking)
for (i in 1:length(esets.debulking.f)) esets.debulking.f[[i]]$debulking <- as.factor(esets.debulking.f[[i]]$debulking)
@

<<debulkingwithbonome,results='hide'>>=
X <- esets.debulking.f$GSE26712_eset
set.seed(1234)
s <- balancedstratification(X=cbind(1:ncol(X)),strata=paste(X$debulking,X$tumorstage),pik=rep(0.5,ncol(X)))
idx.bonometraining <- s==1
esets.debulking.f$GSE26712_eset <- X[, idx.bonometraining]
Y <- X[, !idx.bonometraining]
write.csv(data.frame(GEO=sampleNames(Y), debulking=Y$debulking,
tumorstage=Y$tumorstage),file="output/bonomesamples_NOT_used_for_training.csv")
@

<<cached_medium_debulking_metas34b, cache=TRUE, results='hide'>>=
coefs.debulking.s34b <- metaCMA.coefs(esets.debulking.f, y="debulking", family=binomial)
res.debulking.s34b  <-
metaCMA(esets.debulking.f,y="debulking",coefs=coefs.debulking.s34b, n=200)
res.debulking.s34b.opt <- metaCMA.opt(esets.debulking.f,
coefs=coefs.debulking.s34b,n=200,y="debulking")
final.model.debulking <- res.debulking.s34b.opt$model
@

<<cached_medium_debulking_metas34b_reml, cache=TRUE, results='hide'>>=
tmp <- metaCMA.opt(esets.debulking.f,
coefs=coefs.debulking.s34b,n=200,y="debulking",rma.method="REML")
final.model.debulking.reml <- tmp$model
@

<<cached_medium_debulking_optimizegrid, cache=TRUE,results='hide'>>=
ma.db <- lapply(c(5,10,seq(25,250,25)), function(i)
    metaCMA(esets.debulking.f,coefs=coefs.debulking.s34b,n=i,y="debulking"))
@

<<cached_short_debulking_finalmodelall, cache=TRUE>>=
tmpall.db <- metaCMA.opt(esets.debulking.f,
coefs=coefs.debulking.s34b,n=nrow(esets.debulking.f[[1]]),y="debulking")
@

\begin{figure}
<<correlationriskscores>>=
X <- dfrcboth[,c("risk", "risk_reml", "risk_tcga", "risk_jci")]
colnames(X) <- c("Meta-Analysis\n(FE)", "Meta-Analysis\n(RE)","TCGA", "Verhaak")
ezCor(X, label_size=6)
@
\caption{Pairwise Pearson correlation of the gene signature risk scores for the
Meta-Analysis, TCGA and Verhaak et al. signatures. For the Meta-Analysis
signature, we show risk scores correlation obtained with our default fixed-effects (FE)
meta-analysis and with a random-effects model (RE). Numbers in the
upper-right, tringular half of the matrix are the Pearson correlation
coefficients, which were all statistically significant ($P < 0.05$). Pairwise
scatterplots of the risk scores are shown in the lower-left half and the
risk score histograms are shown on the matrix diagonal.}
\label{fig:cor:risk}
\end{figure}

\clearpage
\section{Comparison with clinical characteristics}

In the main text, we presented a Kaplan-Meier analysis of our risk
stratifications (Figure 4a) and of a multivariable model utilizing these
stratification together with debulking status (optimal vs. suboptimal) and
tumor stage (III vs. IV), the most important and commonly available
characteristics with known survival association. In Supplemental
Table~\ref{tbl:reg:all}, we provide the Cox regression coefficients (the
log-transformed Hazard Ratios) for the multivariable models. For datasets with
available age at time of diagnosis, we included age in the model.

<<tblregall, results='asis'>>=
.getFormula <- function(X, fields=c("strata","debulking_orig", "tumorstage",
"age_at_initial_pathologic_diagnosis")) {
    avl <- sapply(fields, function(x) sum(!is.na(X[[x]]))>0)
    form="y~"
    sep=""
    for (i in 1:length(fields)) { 
        if (avl[i]) form <- paste(form, fields[i], sep=sep)
        sep="+"
    }
    form
}
dfrcboth$tumorstage <- as.factor(dfrcboth$tumorstage)

dfrcboth$batch <- factor(as.character(dfrcboth$batch))

cph.form <- sapply(levels(dfrcboth$batch), function(x)
.getFormula(dfrcboth[dfrcboth$batch==x,]))
res <- lapply(1:length(cph.form), function(i) coxph(as.formula(cph.form[[i]]),
dfrcboth[dfrcboth$batch==levels(dfrcboth$batch)[i],]))

texreg(res, custom.model.names=
    gsub("Yoshihara", "Yosh.",
    gsub("Konstantinopoulos","Konstantin.", gsub(" et al,","", sort(.getDatasetNames(c(esets.f[-tcga_id],
    esets.validation[-4]))))))
    ,
label="tbl:reg:all", tight=FALSE, use.packages=FALSE, float.pos="!htb",
custom.coef.names=c(risk="Gene Signature (Low- vs high-risk)",
"Debulking (subopt. vs. opt.)",
"Stage (IV vs III)","Age"), sideways=TRUE, scriptsize=TRUE, booktabs=TRUE,
caption="Cox regression coefficients for multivariable models utilizing the
gene signature risk stratifications and the available
characteristics with known overall survival association when provided by the
authors. Numbers in brackets are standard errors. The row \\textit{PH Test} reports the p-value indicating
whether the proportional hazards assumption of the model is
violated \\cite{Grambsch:1994}.",
include.aic = FALSE, include.rsquared = FALSE, include.maxrs = FALSE,
include.zph = TRUE)
@

\section{Survival signature pathway analysis}

Pathway analysis was conducted using the Pathway studio 7.1 program (Ariadne
Genomics). All 200 genes in the debulking signature and the overall survival
signature were input into the program for analysis respectively. Pathway
enrichment analysis (build-in algorithm) was used to identify statistically
significant pathways within the signature. A p-value less than 0.05 was
considered significant, indicating the associated pathway identification was
unlikely to be resulted from chance.

Our survival signature did not reveal dominant pathways, unlike the debulking
signature. This is likely related to the more complex and diverse biological
determinant factors for patient overall survival.  Nevertheless, 25 genes in
our 200-gene signature can be enriched through TGF-$\beta$ pathway, suggesting
its poor prognostic impact on patient overall survival (Supplementary Figure
\ref{fig:pathwayos}).  Secondly, 9 genes can be linked to PDGF signaling, in
which the overexpression of both receptor (PDGFA and PDGFB) and ligand (PDGFD)
indicates poor outcome.  The TAF activation in the OS signature, marked by
COL11A1 \cite{Tothill:2008}, is also noticed, considering the synergistic
effects of TFG-$\beta$ and PDGF in TAF activation \cite{Cirri:2012}.
Functionally, 11 genes have been demonstrated to be involved in EMT (SNAI2,
ZEB1, BMI1, C13ORF15, EDNRA/ET-1, PDGFD, SERPINE1/PAI-1, PDPN, CUX1, SERPINA1
and SPDEF), which is closely related to tumor metastasis and chemoresistance
to portend poor prognosis in ovarian cancer. Recently, the emerging roles of
PDGF signaling (especially via PDGFD) in EMT and cancer stem cell maintenance
during tumorigenesis have been described \cite{Devarajan:2012, Wu:2013}. In
addition, the endothelin/ET-1 pathway also positively enhance the expression
of SNAI2, thus EMT \cite{Rosano:2010, Rosano:2011}.

\begin{figure}[!htb]
{\centering \includegraphics[width=0.75\textwidth]{input/figures/pathwayos} 

}
\caption{Pathway analysis of the OS signature. A gene is labeled in red when it
portends poor prognosis. Conversely, genes indicate favorable outcome are
labeled in blue. Red broken arrows: direct stimulatory modification; Orange
arrows: TGF-$\beta$ signaling activated transcriptional regulations; Blue solid
arrows: PDGF signaling activated transcriptional regulations; Green arrows:
other direct regulations.}
\label{fig:pathwayos}
\end{figure}

\section{Meta-analysis is superior to single study training}
The training sample size was positively correlated with accuracy of patient
risk stratifications, up to the maximum training sample sizes of 1,250
(Supplemental Figure~\ref{fig:samplesize}). This finding demonstrated that (i)
a meta-analysis of microarray datasets even from different platforms is
superior compared to single study training and (ii) our leave-one-dataset-out
approach, in which we removed training datasets to obtain unbiased estimates
of the final signature's HR, is not an over-optimistic estimate, because
removing training datasets as expected made the signature worse.

<<cached_medium_datasetshr2prepare, cache=TRUE>>=
.allCombsEval <- function(ids, esets, object) {
       lodo <- .lodocvPlot(esets, models=object$fits[[1]]$fit, ids=ids,
       plot=FALSE)[[1]]
       fit <- coxph(lodo$y~lodo$strata)
       summary(fit)$coefficients
}
esets.fx <- c(esets.f, GSE32062.GPL6480_eset=esets.validation$GSE32062.GPL6480_eset)
esets.fx <- metaCMA.common.gene.esets(esets.fx)
coefs.fx <- metaCMA.coefs(esets.fx)
@

<<cached_long_datasetshr2, cache=TRUE>>=
ret.hr <- lapply(1:length(esets.fx), metaCMA.allcombinations,
esets=metaCMA.censor(esets.fx, censor.at=365.25*5), coefs=coefs.fx, n=200,eval.fun=.allCombsEval )
@

<<plotdatasetshrprepare>>=
df.hr <- do.call(rbind, lapply(1:length(esets.fx),
function(i) data.frame(.getDatasetNames(esets.fx)[i], unlist(ret.hr[[i]]$n),
  sapply(ret.hr[[i]]$evaluation, function(y) 1/y[2]))))
 
colnames(df.hr) <-c("Dataset", "n", "HR")

training <- c(!sapply(esets.fx[1:8], .defaultFilter), GSE32062.GPL6480_eset=FALSE)

.psContains <- function(i, esets) {

    .doPS <- function(ps) {
        idx <- 1:length(esets) %in% c(i,(1:length(esets))[-i][ps])
        allt <- sum(!training[idx]) == 0 && sum(training[!idx])==0
        idx <- 1:length(esets) %in% (1:length(esets))[-i][ps]
        allt <- allt | sum(!training[idx]) == 0 && sum(training[!idx])==0

        ifelse(allt, "Yes", "No")
     }
     pss <- metaCMA.powerset(length(esets[-i]))
     ret <- lapply(pss, .doPS)
}

df.hr$meta <- unlist(lapply(1:length(esets.fx), .psContains, esets.fx))

df.hr$meta[df.hr$meta == "Yes"] <- "6 largest datasets"

#add original TCGA model

df.hr$size <- ifelse(df.hr$meta=="No", 1,2)
df.hr$meta[df.hr$meta == "No"] <- "Others"

df.hr$meta <- factor(df.hr$meta, levels=c("6 largest datasets", 
"Others"))

#df.hr <- df.hr[df.hr$meta=="Yes",]

p <-ggplot(df.hr, aes(n,HR))
p <- p +geom_point(aes(col=meta,size=size))
p <- p +facet_wrap(~Dataset)+coord_cartesian(ylim= c(0.75,3.25))+ylab("Hazard Ratio")+xlab("Total Training Sample Size")
p <- p+ scale_size( name = "Training datasets", guide="none")
p <- p+ scale_colour_discrete( name = "Training datasets")
p <-p +geom_smooth(method="lm")
@

\begin{figure}
<<plotdatasetshr>>=
facetAdjust(p+theme_classic2(14)+theme(axis.text.x = element_text(angle = 45,
hjust = 1),legend.position="bottom"))
@
\caption{Prediction accuracy as a function of training sample sizes. This plot
shows the improvement of predictions when training samples sizes were
increased. For each of the 9 shown datasets, 255 different models were trained
using the remaining 8 datasets only. These 255 ($2^8-1$) models correspond to
all possible combinations of the 8 remaining datasets. Each point in the plot
represents a training dataset combination and the combination's total sample
size is shown on the x-axis, its Hazard Ratio (HR) in the validation data on
the y-axis. The results of our training datasets, the 6 large studies published
before March 2012, are marked with a red dot. This analysis showed that increasing the sample size via
meta-analysis typically increased the model HRs. TCGA data was further
identified as a difficult validation dataset. }
\label{fig:samplesize}
\end{figure}

\clearpage

\section{Comparison with the Berchuck 2004 debulking signature}

We generated a gene signature for suboptimal debulking surgery and validated
this signature by \textit{leave-one-dataset-out} cross-validation.  The
meta-analysis summary is shown in Supplemental
Figure~\ref{fig:roc:debulking} as ROC curves.

<<debulking_meta>>=
preds <- lapply(res.debulking.s34b$fits,function(fit) fit$risk@lp)
preds.postn <- lapply(esets.debulking.f, function(X) exprs(X)["POSTN",])
labels <- lapply(esets.debulking.f, function(X) X$debulking)
titles <- .getDatasetNames(esets.debulking.f)
idx.debfi <- !sapply(esets.debulking.f,.debulkingFilter)
@

<<cached_long_debulking_rankprod, cache=TRUE, results='hide'>>=
set.seed(1234)
res.debulking.rp <-
metaCMA.rankproduct(esets.debulking.f[!sapply(esets.debulking.f,.defaultFilter)], y="debulking",
treatment.label="suboptimal", huge=TRUE)
@


\begin{figure}
<<cached_short_debulking_metaplot, cache=TRUE>>=
aucs.db <- .plotROCpanel(preds[idx.debfi],labels[idx.debfi],titles[idx.debfi],3,3)
@
\caption{Prediction of suboptimally debulked tumors in a leave-one-dataset-out
cross-validation. The prediction model calculates for each sample a score. The
higher the score, the higher the probability the tumor will be not optimally
debulkable. For each dataset, the model is trained using only the remaining
datasets. ROC curves visualize the true and false positive rates as a function of
the probability cutoffs. Datasets with sample size smaller than 75 were not
used for training. AUCs statistically significantly ($P < 0.05$) larger than
0.5 are marked with an asterisk.}  
\label{fig:roc:debulking}
\end{figure}

\begin{figure}
<<cached_short_debulking_berchuck04, cache=TRUE>>=
berchuck04 <- read.csv("input/berchuck04.csv",as.is=TRUE)
berchuck04.coefs <- berchuck04[,2]
names(berchuck04.coefs) <- berchuck04[,1]
# this uses a simple weighted average of the up minus the down regulated genes to calculate patient risk scores
model.berchuck04 <- new("linearriskscore", coefficients=berchuck04.coefs)
preds.berchuck04 <- lapply(esets.debulking.f, function(X) predict(model.berchuck04, newdata=X)@lp)
aucs.db.berchuck <-
.plotROCpanel(preds.berchuck04[idx.debfi][-7],labels[idx.debfi][-7],titles[idx.debfi][-7],3,3)
@
\caption{Prediction of debulking status with the Berchuck et al. signature
\cite{Berchuck:2004} as in Supplemental Figure~\ref{fig:roc:debulking}. The
Dressman data was excluded because a subset of Dressman samples was used for
training.}
\label{fig:roc:debulking:berchuck04}
\end{figure}

\begin{figure}
<<cached_short_debulking_postn, cache=TRUE>>=
aucs.postn <- .plotROCpanel(preds.postn[idx.debfi],labels[idx.debfi],titles[idx.debfi],3,3)
@
\caption{Prediction of debulking status with the POSTN expression alone
as in Supplemental Figure~\ref{fig:roc:debulking}.}
\label{fig:roc:debulking:postn}
\end{figure}


<<writemodelobjs>>=
save(final.model, final.model.debulking, file="output/models.rda")
@

We compared our results to the only published model predicting debulking
success \cite{Berchuck:2004} we were aware of, with the corresponding ROC
curves shown in Supplemental Figure~\ref{fig:roc:debulking:berchuck04}.
Because Berchuck et al. did not provide the coefficients of their model, we
subtracted the average expression of genes down-regulated in suboptimal from
the average of up-regulated genes. The authors further did not specify the
exact probe sets and provided Unigene or Genbank accession numbers for only a
subset of genes in their signature. We manually tried to identify current HGNC
symbols for the genes. We could map 21 of the 32 genes utilized in their model
to probes used in curatedOvarianData. None of these 21 genes overlapped with
our two signatures. 

In Supplemental Table~\ref{tbl:reg:debulking}, we show the logistic regression
coefficients of the gene signature risk scores adjusted for FIGO stage (III
vs. IV). 

<<cached_short_debulking_adjust,results='asis', cache=TRUE>>=
dfrc.debulking <- .createClinical(esets.debulking.f[idx.debfi])
dfrc.debulking$risk <- unlist(preds.berchuck04[idx.debfi])
dfrc.debulking$risk[grepl("Dressman",dfrc.debulking$batch)] <- NA

fit.debulking.berchuck04 <-
    glm(debulking~risk+tumorstage,dfrc.debulking,family="binomial")

dfrc.debulking$risk_berchuck04 <- dfrc.debulking$risk
dfrc.debulking$risk <- unlist(lapply(res.debulking.s34b$fits[idx.debfi], function(X)
X$risk@lp))

fit.debulking <-
    glm(debulking~risk+tumorstage,dfrc.debulking,family="binomial")

texreg(list(fit.debulking,fit.debulking.berchuck04), custom.model.names=c("Meta-Analysis", "Berchuck et al. 2004"),
caption="Prediction of debulking status. The table lists the
regression of our leave-one-dataset-out cross-validated meta-analysis
debulking gene signature
signature and the signature published by Berchuck et al. \\cite{Berchuck:2004}. The predictions were adjusted for tumor stage.
Numbers in brackets are standard errors.",
label="tbl:reg:debulking", tight=FALSE, use.packages=FALSE, float.pos="!htb",
booktabs=TRUE,
custom.coef.names=c("Intercept", "Gene Signature","Stage (III vs IV)"), 
include.aic = FALSE, include.bic = FALSE, include.deviance = FALSE, include.loglik = FALSE)
@

<<cached_short_debulking_berchuck04_aucdiff,cache=TRUE>>=
aucdiff <- roc.test(roc(dfrc.debulking$debulking, dfrc.debulking$risk_berchuck04), roc(dfrc.debulking$debulking, dfrc.debulking$risk),alternative="less")
@

ROC curves for public microarray data of our top-ranked hit \textit{POSTN} are
shown in Supplemental Figure~\ref{fig:roc:debulking:postn}. 

\clearpage


\section{Fixed- vs. random-effects debulking signature}

<<cached_short_debulking_fevsre, cached=TRUE>>=
fe.model <- final.model.debulking
re.model <- final.model.debulking.reml
a <- metaCMA.compare(fe.model, re.model, esets.debulking.f, coefs=coefs.debulking.s34b,
y="debulking")
@

<<debulking_fevsre_cnt>>=
hetgenes <- sapply(a$genes, function(g) a$fe$rma[[g]]$QEp)
hetgenes <- names(hetgenes[hetgenes < 0.05])
nhet <- length(hetgenes) 
nint <- length(intersect(names(fe.model@coefficients),
    names(re.model@coefficients)))
nunion <- length(union(names(fe.model@coefficients),
    names(re.model@coefficients)))
@

As in section \ref{sec:fixed}, we examined the overlap of the debulking gene
signatures obtained with a fixed- and a random-effects gene ranking
(Supplemental Figure~\ref{fig:effects:db:venn}). We further compared both
fixed-effects and random-effects signature with a signature obtained with the
Rank Products method \cite{Breitling:2004}, a frequently used alternative
microarray meta-analysis method. The Rank Products method currently does not
support censored outcome and could not be compared to the overall survival
signatures.  The Rank Products signature was more similar to the fixed-effects
(overlap of 114 genes) than to the random-effects signature (overlap of 76
genes), most likely because Rank Product also weights datasets strictly
according their sample size.

We observed a lower number of genes with marked heterogeneity in the
fixed-effects signature (\Sexpr{nhet} genes) compared to the overall survival
signature. The Yoshihara 2010 and the Bentink et al. datasets were again the
most common source of heterogeneity, albeit less clear than for the overall
survival signature (Supplemental
Figures~\ref{fig:effects:db:hm1}-\ref{fig:effects:db:hm3}).

Integrative correlation analysis \cite{Parmigiani:2004} showed that there were
no systematic differences between the genes with statistically significant
heterogeneity and the ones without (Supplemental
Figure~\ref{fig:effects:db:intcor}).  We expect, however, that the largest
datasets (most importantly the Bonome dataset published from our lab and the
TCGA samples) are similar to our validation samples in terms of surgical
procedures and pathologic review.  Accounting for heterogeneity with a
random-effects model may overly weight smaller studies for which we have only
limited surgery information.

\begin{figure}[!htb]
\centering
\subfigure[Venn diagram signature overlaps]{
\label{fig:effects:db:venn}
<<debulking_fevsre_plotting_venn, out.width="0.38\\textwidth">>=
final.signature.debulking.rp <-
names(head(sort(apply(res.debulking.rp[[2]],1,min)),200))
venn(list("Random-Effects"=names(re.model@coefficients),"Fixed-Effects"=names(fe.model@coefficients),
  "Rank Product"=final.signature.debulking.rp))

@
}
\subfigure[Comparison of pooled regression coefficients]{
\label{fig:effects:db:coef}
<<debulking_fevsre_plotting_corcoef, out.width="0.38\\textwidth">>=
 plot(sapply(a$fe$rma[a$genes], function(x) x$b), sapply(a$re$rma[a$genes],
 function(x) x$b), xlab="Pooled Cox regression coefficients (fixed-effects)",
 ylab="Pooled Cox regression coefficients (random-effects)", las=1,
 cex.axis=1.4, cex.lab=1.45 )
@
}\\
\subfigure[Genes only in FE signature]{
\label{fig:effects:db:hm1}
<<debulking_fevsre_plotting_1, out.width="0.38\\textwidth">>=
difffe <- setdiff(names(fe.model@coefficients), names(re.model@coefficients))
difffe <- difffe[ difffe %in% rownames(coefs[[1]])]

cols <- ifelse(difffe %in% a$genes[sapply(a$genes, function(g) a$fe$rma[[g]]$QEp) < 0.05], "black", "white")
idx <- !sapply(esets.f, .defaultFilter)
x <- coefs[[1]][difffe,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}
\subfigure[Genes only in RE signature]{
\label{fig:effects:db:hm2}
<<debulking_fevsre_plotting_2, out.width="0.38\\textwidth">>=
diffre <- setdiff(names(re.model@coefficients), names(fe.model@coefficients))
diffre <- diffre[ diffre %in% rownames(coefs[[1]])]

cols <- ifelse(diffre %in% a$genes[sapply(a$genes, function(g) a$re$rma[[g]]$QEp) < 0.05], "black", "white")
x <- coefs[[1]][diffre,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}\\
\subfigure[Genes in both FE and RE signature]{
\label{fig:effects:db:hm3}
<<debulking_fevsre_plotting_3, out.width="0.38\\textwidth">>=
intersectrefe <- intersect(names(re.model@coefficients), names(fe.model@coefficients))
intersectrefe <- intersectrefe[ intersectrefe %in% rownames(coefs[[1]])]

cols <- ifelse(intersectrefe %in% a$genes[sapply(a$genes, function(g) a$re$rma[[g]]$QEp) < 0.05], "black", "white")
x <- coefs[[1]][intersectrefe,idx]
colnames(x) <- paste(.getDatasetNames(esets.f[idx]),"  ")
heatmap.3(x, margins=c(10,5), RowSideColors=cols, col.srt=45)
@
}
\subfigure[Integrative Correlation Analysis]{
\label{fig:effects:db:intcor}
<<debulking_intcorplot, out.width="0.38\\textwidth">>=
pdiff <- t.test(c( ic1[diffre],  ic2[diffre])  ,c( ic1[hetgenes]
    ,ic2[hetgenes]  ))$p.value

d.f <- rbind( data.frame(IntCor=ic1[diffre], Dataset=.getDatasetNames(esets.f)[1],
Method="RE"),
data.frame(IntCor=ic1[hetgenes], Dataset=.getDatasetNames(esets.f)[1],
Method="FE"),
data.frame(IntCor=ic2[diffre], Dataset=.getDatasetNames(esets.f)[3],
Method="RE"),
data.frame(IntCor=ic2[hetgenes], Dataset=.getDatasetNames(esets.f)[3],
Method="FE"))
d.f$Method <- factor(d.f$Method, levels=c("FE", "RE"))
ggplot(d.f,aes(Method,IntCor,fill=Dataset))+geom_boxplot()+
ylab("Integrative Correlation")+theme_classic2(15)

@
}
\caption{Comparison of a fixed- (FE) and a random-effects (RE) ranking of genes
associated with debulking status. See main text as well as Supplemental
Figure~\ref{fig:effects} and accompanying text for description of the panels.}
\end{figure}

\clearpage

\section{Debulking gene signature size}

\begin{figure}[!htb]
<<cached_short_debulking_sigsize, out.width='0.75\\textwidth', cache=TRUE>>=
res <- lapply(1:length(ma.db), function(i)
.plotROCpanel(preds=lapply(ma.db[[i]]$fits, function(x) x$risk@lp)[idx.debfi],
labels=lapply(esets.debulking.f[idx.debfi], function(x)
x$debulking),nrow=3,ncol=3,titles=.getDatasetNames(esets.debulking.f)[idx.debfi],plot=FALSE))

yi <- lapply(res, function(x) x$yi)
facetAdjust(.plotN(esets.debulking.f[idx.debfi], yi, ylab="AUC"))
@
\caption{We used for all our gene signatures a fixed gene signature size of
200 genes. Here we show the influence of this cutoff on the prediction
accuracy of the debulking signature. Each point represents the AUC of a model
with $x$ genes in the corresponding dataset that was trained using the
remaining datasets only. For the Bonome et al. dataset, this shows the
leave-one-dataset-out cross-validated performance in the 93 samples used for
training. See main text for the AUCs of the Bonome validation Affymetrix and
qRT-PCR data.}
\label{fig:cutoff:debulking}
\end{figure}

\clearpage
\section{IHC and qRT-PCR validation of genes associated with surgery outcome}

\subsection{Gene selection criteria}

qRT-PCR was used to validate as best as possible the signature because the
technique is more quantitative than arrays. Therefore, the genes were selected
based upon the proposed biology and focused upon the identified pathways.
Except for PTCH1, which was selected to verify the contribution of
hyperactivated Hedgehog pathway, a potent metastasis and EMT inducer
\cite{Li:2012}, all genes selected were involved related
to TGF-$\beta$ signaling (increased TGFBR2 as one of the potential modulators,
Figure 5). In addition to their direct oncogenic roles, POSTN and FAP have
been demonstrated as markers of the activation of tumor associated
fibroblasts, which play a definite role in tumor growth and metastasis
\cite{Cirri:2012}. Although less details were known about the function of
CXCL14, NUAK1 and TNFAIP6, all three genes have been shown as positive
regulators of tumor metastasis \cite{Wente:2008, Chang:2012, Schuetz:2006}. 
Additionally, NUAK1 mediates TGF-$\beta$ induced hypoxia
tolerance \cite{Suzuki:2005}, which is critical for both metastatic
initiation and the adaption of micrometastases. Upregulated CXCL14 upon TAF
activation further enhances the potential of TAFs to support tumor metastasis
and angiogenesis \cite{Augsten:2009}. Furthermore, the expression of
CXCL14, NUAK1 and TNFAIP6 all predicts poor prognosis as revealed by our
meta-analysis, indicating the potential importance of these genes in ovarian
tumor biology. Finally, all selected genes encode secreted proteins or kinases
and are thus `targetable'. We expect the validation will contribute
the design of novel targeted therapeutic regime to improve the rate of optimal
debulking.

For IHC candidate selection, POSTN and CXCL14 are the two genes ranked the
highest by fold-change and FDR in the debulking signature for which
there were commercially available antibodies which had been used in published
studies. In addition to these, we selected an additional protein target which
could be used as evidence of pathway activation. Phospho-Smad2/3 was selected
to confirm the TGF-$\beta$ hyperactivation in tumors which could not be optimally
debulked. 

\subsection{IHC Scoring}

Scoring of the IHC level of each protein was conducted by determining the
percentage and intensity of positive cells in three different areas at 100x
magnification for each tissue core. First, the percentage of positive cells in
each section was scored with a 5-point scale: 0 for <5\%, 1 for 5-25\%, 2 for
26- 50\%, 3 for 50-75\%, and 4 for over 75\%.  Second, the intensity of
positive signal was scored with a 3-point scale: 1 for weak staining, 2 for
moderate staining, and 3 for intense staining. The weighed score of staining
intensity of each section was obtained by multiplying the percentage score by
the intensity score (the maximum weighed score is 12). Since each sample was
represented by two tissue cores, the average score was calculated for the IHC
intensity of each sample (listed in Table S1). A categorical system was then
applied as follows: Class 0 (-) score 0-3; Class 1 (+) score 4-6; Class 2 (++)
score 7-9; Class 3 (+++) score 9-12.  The IHC was scored by two individuals
including one pathologist for independent IHC scoring. The scoring was done
blinded to the clinical data.

\subsection{Multivariable Models}

As regression coefficients obtained from microarray data are not directly
translatable to qRT-PCR or IHC measurements, we only used the signs of the
coefficients from the microarray-based signature. This means that all genes were
equally weighted, with the expression levels of down-regulated genes in
suboptimal subtracted from the ones of up-regulated genes. Group sizes for
patient stratification in high-, medium- and low-risk corresponded to the
numbers of suboptimal and optimal tumors for high- and low-risk, respectively.
The 33\% of high-risk samples with lowest risk and the 33\% of low-risk samples
with highest risk were then classified as medium-risk.

In Supplemental Table~\ref{tbl:coefficients:ihc}, we show that POSTN, pSmad2/3
and CXCL14 are predictors debulking surgery outcome independent of tumor grade
and stage. In the multivariable models with all three proteins, both adjusted
and unadjusted for stage and grade (shown in columns 1 and 2 of Supplemental
Table~\ref{tbl:coefficients:ihc}), inclusion of CXCL14 could not further
improve the model.

We further validated selected genes by qRT-PCR in 78 Bonome samples not used for
training. In Figure~\ref{fig:qrtpcr:cor}, we show the Spearman correlation of the
qRT-PCR expression values and the Affymetrix signal intensities.

<<ihcpostn,fig.width=3.5,fig.height=3.5,include=FALSE>>=
ihc <- read.xls("input/IHC.xls", as.is=TRUE,  na.strings = "missing")

ihc$Stage <- ihc$FIGO.Stage
ihc$Stage[grepl("III", ihc$FIGO.Stage)] <- 3
ihc$Stage[grepl("IV", ihc$FIGO.Stage)] <- 4
ihc$Stage[grepl("II?", ihc$Stage)] <- 2
ihc$Stage[ihc$Stage==""] <- NA
ihc$Stage <- as.numeric(ihc$Stage)
colnames(ihc)[4:6] <- c("pSmad23", "POSTN", "CXCL14")
ihc$Grade <- ihc$GRADE
ihc$Debulking <- as.factor(ihc$Debulking)

.plotROC(ihc$POSTN, ihc$Debulking)
#summary(glm(as.factor(Debulking)~as.factor(GRADE)+as.factor(FIGO.x),ihc, family="binomial"))
@

<<ihcpsmand,fig.width=3.5,fig.height=3.5,include=FALSE>>=
.plotROC(ihc$pSmad2, ihc$Debulking)
@

<<ihccxcl14,fig.width=3.5,fig.height=3.5,include=FALSE>>=
.plotROC(ihc$CXCL14, ihc$Debulking)
@

<<ihcmultivariable,fig.width=3.5,fig.height=3.5,include=FALSE>>=
.plotROC(ihc$POSTN+ihc$pSmad2+ihc$CXCL14, ihc$Debulking)
@

<<stratification, results='hide'>>=
# Calculate accuracy of risk stratification
x <- ihc$pSmad23 + ihc$POSTN + ihc$CXCL14
idx <- !is.na(x)
counts <-sapply(levels(ihc$Debulking), function(i) sum(ihc$Debulking==i))
counts <- counts/sum(counts)
cutoff <- c(counts[1]*2/3, counts[1]+counts[2]*1/3)
p <- quantile(x, cutoff, na.rm=TRUE)
table(as.factor(sapply(x[idx], function(y) ifelse(y <= p[1], "low",
ifelse(y <= p[2], "mid", "high")))), ihc$Debulking[idx])
@

<<models,results='asis'>>=

fit.adj <- glm(as.factor(Debulking)~POSTN+pSmad23+CXCL14+Grade+Stage,
    data=ihc, family="binomial")

fit <- glm(as.factor(Debulking)~POSTN+pSmad23+CXCL14,
    data=ihc, family="binomial")

fit.POSTN <- glm(as.factor(Debulking)~POSTN+Grade+Stage,
    data=ihc, family="binomial")

fit.pSmad23 <- glm(as.factor(Debulking)~pSmad23+Grade+Stage,
    data=ihc, family="binomial")

fit.CXCL14 <- glm(as.factor(Debulking)~CXCL14+Grade+Stage,
    data=ihc, family="binomial")

texreg(list(fit, fit.adj, fit.POSTN, fit.pSmad23,
    fit.CXCL14),custom.model.names=c("IHC", "IHC adj.", "POSTN adj.", 
    "pSmad2/3 adj.", "CXCL14 adj."), 
    caption=paste("Multivariable prediction of debulking status.", 
    "Shown are multivariable models based on IHC staining only and",
    "models adjusted (adj.) for tumor stage (III vs. IV) and grade (2 vs. 3).",
    "Numbers in brackets are standard errors."), booktabs=TRUE,
    label="tbl:coefficients:ihc", use.packages=FALSE, float.pos="!htb",
include.aic = FALSE, include.bic = FALSE, include.deviance = FALSE,
include.loglik = FALSE)

@

<<qrtpcr, fig.width=3.5,include=FALSE>>=
qrtpcr <- read.xls("input/qrtpcr.xls", as.is=TRUE)
M <- t(as.matrix(qrtpcr[,9:ncol(qrtpcr)]))
colnames(M) <- qrtpcr[,3]

eset.qrtpcr <- ExpressionSet(impute.knn(M)$data,
    phenoData=phenoData(esets.debulking$GSE26712_eset[, colnames(M)]))

eset.qrtpcr$grade <- qrtpcr$Grade

coefficients <-
na.omit(final.model.debulking@coefficients[featureNames(eset.qrtpcr)])

model.qrtpcr <- new("linearriskscore", coefficients=sign(coefficients),
modeltype="compoundcovariate")

.plotROCpanel(list( 
    predict(model.qrtpcr,newdata=esets.debulking$GSE26712_eset[,
        sampleNames(eset.qrtpcr)])@lp,
    predict(model.qrtpcr,newdata=eset.qrtpcr)@lp,
    exprs(eset.qrtpcr)["POSTN",]),
# should be the same
list(as.factor(esets.debulking$GSE26712_eset[,
sampleNames(eset.qrtpcr)]$debulking),
as.factor(eset.qrtpcr$debulking), 
as.factor(eset.qrtpcr$debulking)), 
titles=c("Affymetrix", "qRT-PCR", "POSTN"), nrow=3, ncol=1)
@

<<postnkm,fig.width=4.9,fig.height=4.9, include=FALSE>>=
idx <- sapply(esets.validation, function(X) "POSTN" %in% featureNames(X))

Xs.all <- c(esets.f, esets.validation[idx])

tmp1 <- unlist(lapply(Xs.all, function(X) { x<- exprs(X)["POSTN",]; ifelse(x <
median(x),"Low Expression", "High Expression") } ))
tmp.df <- .createClinical(Xs.all)
summary(coxph(y~as.factor(tmp1)+debulking+tumorstage, tmp.df))
@


\begin{figure}
<<debulking_corplot>>=
data(GSE26712_eset, package="curatedOvarianData")
tmp.df <- do.call(rbind, lapply(featureNames(eset.qrtpcr), function(g)
    data.frame(Gene=g,
        Affymetrix=exprs(GSE26712_eset)[g,sampleNames(eset.qrtpcr)],
        qRTPCR=exprs(eset.qrtpcr)[g,],Debulking=eset.qrtpcr$debulking)))

c.tests <- lapply(featureNames(eset.qrtpcr), function(g) 
    cor.test(exprs(GSE26712_eset)[g,sampleNames(eset.qrtpcr)],
        exprs(eset.qrtpcr)[g,], method="spearman"))

rhos <- data.frame(Affymetrix=8.7, qRTPCR=0.1, rho=sapply(c.tests,
    function(x) paste("rho==",round(x$estimate, digits=2), sep="")),
        Debulking=NA, Gene=featureNames(eset.qrtpcr))

facetAdjust(ggplot(tmp.df, aes(Affymetrix,
  qRTPCR,colour=Debulking))+geom_point()+facet_wrap(~Gene)+scale_y_log10(breaks =
  trans_breaks("log10", function(x) 10^x),labels = trans_format("log10",
  math_format(10^.x)))+geom_text(aes(label=rho),data=rhos, hjust=0,
  parse=TRUE,  show_guide=FALSE)+theme_classic2()+ylab("qRT-PCR"))
@
\caption{Validation of selected genes by qRT-PCR in the Bonome validation
dataset, a subset of 78 samples (39 optimal and 39 suboptimal tumors). Points
represent patients, the x-axis shows the Affymetrix fRMA normalized intensities,
the y-axis the qRT-PCR level. The Spearman correlation of platforms was
statistically highly significant for all genes ($P < 0.001$).}
\label{fig:qrtpcr:cor}
\end{figure}


<<cached_short_debulking_finalmodel_foldchanges, cache=TRUE>>=
# Use the unscaled data to calculate fold-changes. 
load("input/eset.debulking.rda")
esets.db <- esets
esets.db <- esets.db[!sapply(esets.db, .debulkingFilter)]
esets.db <- .fixTCGA(esets.db)
# use only the half of Bonome used for training
ids <- sampleNames(esets.debulking.f$GSE26712_eset)[
    sampleNames(esets.debulking.f$GSE26712_eset) %in%
    sampleNames(esets.db$GSE26712_eset)]

esets.db$GSE26712_eset <- esets.db$GSE26712_eset[,ids]

resM <- metaCMA.foldchanges(esets.db, final.model.debulking, groups="debulking",
contrasts="suboptimal-optimal")
@

<<cached_short_debulking_finalmodel,cache=TRUE>>=
# Write the final debulking model to a spreadsheet.
sig <- names(final.model.debulking@coefficients)

probesets <- do.call(cbind, lapply(esets.debulking.f, function(X)
 as.character(featureData(X)[sig,]$maxmean_probeset)))

tmp <- data.frame(Genes=sig, probesets, 
"Pooled LogIt Coefficient"=
final.model.debulking@coefficients,
"Mean Optimal"= apply(res.debulking.s34b.opt$train$X[which(res.debulking.s34b.opt$train$y=="optimal"),],2,mean),
"Mean Suboptimal"= apply(res.debulking.s34b.opt$train$X[which(res.debulking.s34b.opt$train$y=="suboptimal"),],2,mean),
 P=head(sort(res.debulking.s34b.opt$pvalues),length(sig)),
 FDR=head(sort(p.adjust(res.debulking.s34b.opt$pvalues,method="BH")),length(sig)), resM)

write.csv(tmp[order(abs(tmp$Weighted.Mean), decreasing=TRUE),],
 file="output/final_signature_debulking.csv",row.names=FALSE)
#write.table(res.debulking.s34b.opt[order(abs(res.debulking.s34b.opt$Weighted.Mean), decreasing=TRUE),],
# file="output/final_signature_debulking.tsv",row.names=FALSE,sep="\t")
@
\clearpage
\section{Role of POSTN in ovarian cancer}

The rationale underlying the debulking signature is that the biology of the
tumor contributes the ability to cytoreduce a patient. Even with large tumor
burdens, the frequent confinement of ovarian cancer to the abdominal cavity
makes it amenable to debulking.  Therefore, optimal debulking is more likely to
be achieved in cancers that are inherently less disseminative. Enhanced tumor
metastasis may increase the chance of surgically inaccessible tumors and thus
lower the rate of optimal debulking. The debulking signature developed in this
study reveals the co-activation of TGF-$\beta$, Ras/MAPK/EGR-1 and Hedgehog pathways
as well as stromal (TAF) activation in suboptimal debulked tumors. These
changes may directly contribute to malignant phenotypes with elevated EMT,
extracellular matrix remodeling and pro-angiogenic activity to facilitate tumor
dissemination (especially parenchymal metastases) and thereby render the
hindrance for surgical resection.  POSTN is known to positively drive each step
of tumor metastasis (migration, invasion, angiogenesis, formation of the
metastatic niche, and the growth of the micrometastases). 

POSTN is a secreted cell adhesion protein belonging to the superfamily of
TGF-$\beta$-inducible proteins \cite{Horiuchi:1999}. Periostin binding to the
integrins ($\alpha$v$\beta$3, $\alpha$v$\beta$5, and $\alpha$6$\beta$4)
directly or indirectly (by recruiting RTKs such as EGFR) activates the Akt-
and FAK-mediated signaling pathways, leading to increased cell survival,
angiogenesis, migration, invasion, and importantly, epithelial-mesenchymal
transition of carcinoma cells \cite{Morra:2011}. In addition to the metastatic
initiation, POSTN is also important in the maintenance of metastatic niche for
the efficient colonization, growing and vascularization of micrometastases, a
speed-limit step for tumor dissemination into second sites
\cite{Soikkeli:2010, Malanchi:2012}. POSTN is frequently overexpressed in
ovarian cancer.  The therapeutic potential of POSTN has been demonstrated in
an orthotropic mouse xenograft model for ovarian cancer, through a
neutralizing antibody mediated substantial reduction of tumor burden and the
number of tumor foci \cite{Zhu:2011}.

\clearpage
\bibliographystyle{plain}
\bibliography{metasig}
\newpage
\clearpage
\appendix


<<comptcga,fig.width=5.33,fig.height=8,out.width="0.467\\textwidth",out.height="0.7\\textwidth",results='hide',include=FALSE>>=

X.td <- dfrcboth[!is.na(dfrcboth$tumorstage) & !is.na(dfrcboth$debulking_orig),]

fit.noclinical <- coxph(y~strata,X.td)
fit.clinical <- coxph(y~tumorstage+debulking_orig,X.td)
fit <- coxph(y~strata+tumorstage+debulking_orig,X.td)
fit.jci <-
coxph(y~strata_jci+tumorstage+debulking_orig+Immunoreactive+Mesenchymal+Proliferative+Differentiated,X.td)

set.seed(1234)
yhat <- cvRisk(fit, X.td)
yhat.jci <- cvRisk(fit.jci, X.td)
yhat.clinical <- cvRisk(fit.clinical, X.td)

par(mfrow=c(3,2))
par(mar=c(4.5, 4.1, 2.5, 1.5))
plotKM(y=dfrcboth$y, strata=dfrcboth$strata,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="A)   Meta-Analysis Signature")
plotKMStratifyBy("median",y=X.td$y,linearriskscore=yhat,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="B)  Signature + Stage + Debulking")
plotKMStratifyBy("median",y=X.td$y,linearriskscore=yhat.clinical,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="C)  Stage and Debulking only")
plotKM(y=dfrcboth$y, strata=dfrcboth$strata_tcga,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="D)   TCGA Signature")
plotKM(y=dfrcboth$y, strata=dfrcboth$strata_jci,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="E)   Verhaak et al. Signature")
plotKMStratifyBy("median", y=X.td$y, linearriskscore=yhat.jci,
        censor.at=365.25*5,
        show.n.risk=TRUE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.0,
        show.PV=FALSE,cex.n.risk=0.7,cex.lab=0.9,
        xlab="Time (Days)",
        cex.base=1.4,main="F)   Verhaak et al. Multivariable")

@

<<fpcomptcga,fig.width=7.75,fig.height=11.5,out.width="0.4\\textwidth",out.height="0.7\\textwidth",results='hide',include=FALSE>>=
Xs.all <- c(esets.f[-tcga_id], esets.validation[-4])
names(Xs.all) <- .getDatasetNames(Xs.all)

res.strata <- lapply(c(res.lodo, res.val[-4]), function(x) x$strata)
res.strata.tcga <- lapply(c(res.lodo.tcga, res.val.tcga[-4]), function(x) x$strata)
res.strata.jci <- lapply(c(res.lodo.jci, res.val.jci[-4]), function(x) x$strata)

x <- metaCMA.forest.models(metaCMA.censor(Xs.all,censor.at=365.25*5),
risks=list(res.strata, res.strata.tcga, res.strata.jci),
 graphwidth=unit(2.5,
 "inches"),concordance=FALSE,inverse=TRUE,x.ticks=c(0.5,1,2,4,8),clip=log(c(0.5,8)),
 cols=c("darkblue","seagreen", "red"))
@

<<cached_medium_hrdiffconfint, cache=TRUE>>=
d.f <- data.frame( do.call(rbind,lapply(metaCMA.censor(Xs.all,censor.at=365.25*5), function(X) X$y)),
strata=dfrcboth$strata, strata_tcga=dfrcboth$strata_tcga)

hrdiffemp <- .boostrapHRs(d.f,n=10000,inverse=TRUE)
hrdiffci <- quantile(hrdiffemp,c(0.025,0.975))
@

<<cached_medium_hrdiffconfintjci, cache=TRUE>>=
d.f <- data.frame( do.call(rbind,lapply(metaCMA.censor(Xs.all,censor.at=365.25*5), function(X) X$y)),
strata=dfrcboth$strata_jci, strata_tcga=dfrcboth$strata_tcga)

hrdiffjciemp <- .boostrapHRs(d.f,n=10000,inverse=TRUE)
hrdiffjcici <- quantile(hrdiffjciemp,c(0.025,0.975))
@

\clearpage
\section{Session Info}
<<sessioninfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\end{document}
