\documentclass{article}
\usepackage[sc]{mathpazo}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]{hyperref}
\hypersetup{pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
\usepackage{verbatim}
\usepackage{subfigure}
\usepackage{booktabs}
\begin{document}

<<setup, include=FALSE, cache=FALSE>>=
# this is equivalent to \SweaveOpts{...}
opts_chunk$set(fig.path='figure/metasig-', fig.align='center',
fig.show='hold',warning=FALSE, echo=FALSE)
options(replace.assign=TRUE,width=90)
@

\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thetable}{S\arabic{table}}
\setcounter{table}{1}

\title{Risk Prediction for Late-stage Ovarian Cancer Using the Corpus of
Published Expression Data - Supplemental Figures}

\author{Markus Riester, Levi Waldron, Aedin Culhane, Lorenzo Trippa, \\
Curtis Huttenhower, Franziska Michor, Giovanni Parmigiani, Michael Birrer}
\maketitle

Code to reproduce all results presented in this paper is
available at \url{https://bitbucket.org/lima1/ovrc4_signew}.

\tableofcontents 

<<load, include=FALSE>>=
library(genefilter)
library(survival)
library(annotate)
library(hgu133a.db)
library(metafor)
library(ggplot2)
library(xtable)
library(survIDINRI)
library(ROCR)
library(RColorBrewer)
library(lattice)
library(maxstat)
library(exactRankTests)
library(pROC)
library(limma)
library(cvTools)
library(rockchalk)

# only to print the version number in the Appendix. We load the data generated
# with the createEsetsList script
library(curatedOvarianData)

source("src/metaCMA.R")
source("src/utils.R")
@

% load the filtered expression data (run ./runEsetList.sh in the input
% directory
<<createesets, cache=TRUE>>=
load("input/eset.scaled.rda")
esets    <- esets[-match("GSE19829.GPL8300_eset",names(esets))]
esets.f  <- metaCMA.common.gene.esets(esets)
@

% we added the Yoshihara data after we finalized our model, so remove it for
% now and use it later for validation
<<addsurvobj, results='hide'>>=
esets.uf <- esets
japan.idx <- match("GSE32062.GPL6480_eset",names(esets.f))
esets.f <- esets.f[-japan.idx]
@

% calculate univariate Cox regression coefficients for each dataset and each
% gene
<<optimizecoefs, cache=TRUE>>=
coefs = metaCMA.coefs(esets.f)
@

% Do our leave-one-dataset-out cross-validation for different gene-signature
% sizes. 
<<optimizegrid, cache=TRUE,results='hide'>>=
ma = lapply(c(5,10,seq(25,250,25)), function(i)
    #metaCMA(esets.f,coefs=coefs,n=i,method="penalizedSurv",fold=5))
    metaCMA(esets.f,coefs=coefs,n=i))
@

\clearpage
\normalsize

\section{Dependency of gene signature size on prediction performance}

In all our analyses, we fixed the gene signature size to 200 genes. This size
was motivated by the fact that this size is sufficiently small to be
practically useful in a clinical test and by the performance of validated and
random signatures (Waldron et al, submitted). It was shown in Waldron et al.
that smaller signatures tend to be less robust than large signatures. Our
algorithm weighs genes according their rank (see Methods).  This means
increasing the signature size is expected to have only limited influence on
the prediction performance at some point as the weight of the genes decreases
consistently. In Supplemental Figure~\ref{fig:cutoff:train} we confirm that
the signature size had only modest impact on the prediction accuracy in our
algorithm, as long as the signature size was larger than 100 genes.  In this
figure, the prediction accuracy is reported with the C-Index metric. The
C-Index is a pairwise comparison of patients, summarizing the fraction of
pairs where the patient predicted to be at higher risk in fact has shorter
survival.  A C-Index of 0.5 would correspond to a random model, and a C-Index
of 1.0 of to a perfect model. Such a perfect model would predict the correct
order in which patients die. 

\begin{figure}[!htb]
<<validate, out.width='0.75\\textwidth', cache=TRUE>>=
.plotN <- function(esets, ma) {
    w <- sapply(esets,ncol)
    cidx = lapply(ma, function(x) sapply(1:length(esets), function(i)
        evaluate(x$fits[[i]]$risk, measure=new("UnoC"),
        newy=esets[[i]]$y,add=list(tau=365.25*4) )))

    dfCV <- stack(as.data.frame(do.call(rbind,cidx)))
    dfCV <- cbind(dfCV, Genes=c(5,10,seq(25,250,25)))
    dfCV$ind <- unlist(lapply(.getDatasetNames(esets), rep, 12))
    ggplot(dfCV, aes(values, Genes))+geom_point()+facet_wrap(~ind) +
    ylab("Number of Genes") + 
    xlab("C-Index (Concordance)") + 
    theme_grey(base_size=14) + 
    theme(axis.text.x=element_text(angle=45,hjust=1))
}
.plotN(esets.f,ma)
@
\caption{In the \textit{leave-one-dataset-out} cross-validation in
\textbf{Figure 1-2}
of the main paper, we used a fixed gene signature size of 200 genes.  Here we
show the influence of this cutoff on the prediction concordance. Each point
represents the prediction concordance of a model with $y$ genes in the
corresponding dataset that was trained using the remaining datasets only.}
\label{fig:cutoff:train}
\end{figure}

% This now uses all datasets for calculating the final model.
<<finalmodel,cache=TRUE>>=
tmp <- metaCMA.opt(esets.f,coef=coefs,n=200)
final.model <- tmp$model
@

% Now load all the other data.
<<loadvaldata,cache=TRUE>>=
load("input/eset.binary.scaled.rda")
esets.binary <- esets
load("input/eset.validation.scaled.rda")
esets.validation_orig <- esets
load("input/eset.allos.scaled.rda")
esets.allos <- esets
load("input/eset.debulking.scaled.rda")
esets.debulking <- esets
@

% Print the model for the supplement
<<tablemodel>>=
sig <- names(final.model@coefficients)
esets.f.all <- esets.f
esets.f.all$GSE32062.GPL6480_eset <- esets.validation_orig$GSE32062.GPL6480_eset

probesets <- do.call(cbind, lapply(esets.f.all, function(X)
 as.character(featureData(X)[sig,]$maxmean_probeset)))

write.csv(data.frame(Genes=sig, probesets, "Pooled Cox Coefficient"=
 final.model@coefficients,
 P=head(sort(tmp$pvalues),length(sig)),
 FDR=head(sort(p.adjust(tmp$pvalues,method="BH")),length(sig))
), file="final_signature.csv",
row.names=FALSE)
@

\section{TCGA subtypes}

We validated a prediction model that was trained on all training datasets except
TCGA in the 4 TCGA subtypes separately. Kaplan-Meier plots for this analysis
are show in Supplemental Figure~\ref{fig:km:tcgasubtypes}.

\begin{figure}[!htb]
<<subtypes, results='hide',out.width="0.7\\textwidth",out.height="0.7\\textwidth">>=
# this was obtained from a Broad mailing list, because the supplement refers
# to an internal link
subtypes <- read.delim("input/TCGA_489_UE.k4.txt")

esets.f$TCGA_eset$subtype <-
    subtypes[match(make.names(substr(esets.f$TCGA_eset$alt_sample_name,1,20)),
    subtypes[,1]),2]

# use the final model on TCGA. The C-Index will be biased, because TCGA was
# used for training. but we will get an idea in which subtype it works best
esets.f$TCGA_eset$risk <- predict(final.model, newdata=esets.f$TCGA_eset)@lp

# fetch the unbiased risk scores from the leave-one-dataset-out
# cross-validation
esets.f$TCGA_eset$risk_unbiased <- ma[[10]]$fits$TCGA_eset$risk@lp

cutpoint <- median(unlist(lapply(esets.f[-match("TCGA_eset",
 names(esets.f))], function(X) predict(ma[[10]]$fits$TCGA_eset$fit,
 newdata=X)@lp))) 

# no generate a forest plot for the unbiased risk scores
esets.tcga.subtypes <- lapply(levels(esets.f$TCGA_eset$subtype),function(s)
    esets.f$TCGA_eset[,!is.na(esets.f$TCGA_eset$subtype) &
    esets.f$TCGA_eset$subtype==s])
names(esets.tcga.subtypes) <- levels(esets.f$TCGA_eset$subtype)
par(mfrow=c(2,2))
strata.subtypes <- lapply(1:length(esets.tcga.subtypes), function(i) plot(ma[[10]]$fits$TCGA_eset$fit,
newdata=esets.tcga.subtypes[[i]], newy=esets.tcga.subtypes[[i]]$y, 
        censor.at = 365.25*5, cutpoints=cutpoint, cex.base = 1.4, show.n.risk
        = FALSE, show.HR = TRUE, show.PV=FALSE, cex.HR=1.1,
        show.legend = FALSE, main = names(esets.tcga.subtypes)[i],
))
@
\caption{Kaplan-Meier analysis of a model trained in all training datasets except TCGA and
then validated in the 4 subtypes separately.}
\label{fig:km:tcgasubtypes}
\end{figure}

\clearpage
\section{Detailed comparison of the novel survival signature with existing
factors and signatures}

We compared our overall survival prediction model to (i) a model using only
stage (III vs. IV) and debulking status (optimal vs.  suboptimal) and to (ii)
the TCGA prediction model as described in \cite{TCGA:2011}. In addition to the
comparison presented in the main paper, we compared models by the Integrated
Discrimination Improvement (IDI) reclassification measure \cite{Pencina:2008}
as implemented in the survIDINRI package \cite{Uno:2012}.  The IDI compares
two models to estimate whether one model provides an advantage over the other;
that is, whether it assigns higher risk to the patients who die within a
specified time compared to the competing model and a lower risk to patients
who do not die within this time frame. 

Finally, we compared the models by their C-Indices.  Statistical significance
of C-index differences between two models was estimated with the survC1 R
package \cite{Uno:2011}. Performance reports of test datasets always
correspond to leave-one-dataset-out cross-validated patient risk scores.

% Validate the TCGA model. Stolen from Levi's paper supplement.
<<loadmodeltcga, cache=TRUE,results='hide'>>=
load("input/21720365-SuppTable_S6.RData")
model.tcga <- model.official
data(TCGA_eset, package="curatedOvarianData")
featureNames(TCGA_eset) <- sub("-", "hyphen", featureNames(TCGA_eset))
TCGA_eset <- TCGA_eset[ ,!is.na(TCGA_eset$days_to_death) &
                                             !is.na(TCGA_eset$vital_status)]
TCGA_eset$y <- Surv(TCGA_eset$days_to_death / 30,
                               TCGA_eset$vital_status == "deceased")

TCGA.validation.eset <- TCGA_eset[ ,TCGA_eset$batch %in% c("17", "18", "19", "21", "22", "24")]
TCGA.training.eset   <- TCGA_eset[ ,TCGA_eset$batch >= 9 & TCGA_eset$batch <=
15 & !is.na(TCGA_eset$batch)]
##Validation set 2:
data(GSE9891_eset, package="curatedOvarianData")
featureNames(GSE9891_eset) <- sub("-", "hyphen", featureNames(GSE9891_eset))
GSE9891_eset <- GSE9891_eset[ ,!is.na(GSE9891_eset$days_to_death) &
                             !is.na(GSE9891_eset$vital_status)]
GSE9891_eset$y <- Surv(GSE9891_eset$days_to_death / 30, GSE9891_eset$vital_status == "deceased")

#Validation set 3, Bonome et al. (2008):
data(GSE26712_eset, package="curatedOvarianData")
featureNames(GSE26712_eset) <- sub("-", "hyphen", featureNames(GSE26712_eset))
GSE26712_eset <- GSE26712_eset[ ,!is.na(GSE26712_eset$days_to_death) &
                               !is.na(GSE26712_eset$vital_status)]
GSE26712_eset$y <- Surv(GSE26712_eset$days_to_death / 30,
                        GSE26712_eset$vital_status == "deceased")
data(PMID17290060_eset, package="curatedOvarianData")
featureNames(PMID17290060_eset) <- sub("-", "hyphen", featureNames(PMID17290060_eset))
PMID17290060_eset <- PMID17290060_eset[ ,!is.na(PMID17290060_eset$days_to_death) &
                                       !is.na(PMID17290060_eset$vital_status)]
PMID17290060_eset$y <- Surv(PMID17290060_eset$days_to_death / 30,
                            PMID17290060_eset$vital_status == "deceased")
@

% Create a data.frame that contains all relevant clinical information and
% the TCGA and our predictions.
<<prepareclinicaldf, cache=TRUE>>=
X <- esets.uf[-japan.idx]
risk <- lapply(X, function(X) predict(model.tcga, newdata=X,
 type="lp")@lp)
for (i in 1:length(X)) {
    X[[i]]$risk_tcga <- risk[[i]]
    X[[i]]$risk <- ma[[10]]$fits[[i]]$risk@lp
}    
X <- X[-grep("TCGA",names(X))]
dfrc <- .createClinical(X)
dfrc$risk_tcga <- unlist(sapply(X, function(x) x$risk_tcga))
dfrc$risk <- unlist(sapply(X, function(x) x$risk))  
idx.stagedeb <- !is.na(dfrc$age) & !is.na(dfrc$debulking)
idx.stagedeb <- !is.na(dfrc$tumorstage) & !is.na(dfrc$debulking)
dfrc$debulking_orig <- dfrc$debulking
dfrc$debulking <- as.numeric(dfrc$debulking)
@

\begin{figure}
<<reclassificationclinical,cache=TRUE,fig.width=12,fig.height=3,out.width="0.9\\textwidth",out.height="0.3\\textwidth",results='hide'>>=
par(mfrow=c(1,3))
.reclassPlot(dfrc[idx.stagedeb,"y"],dfrc[idx.stagedeb,c("tumorstage",
"debulking" )]
, dfrc[idx.stagedeb,c("tumorstage", "debulking","risk")]
, t0=365.25*1, 
title="All Training Datasets")
.reclassPlot(dfrc[idx.stagedeb,"y"],dfrc[idx.stagedeb,c("tumorstage",
"debulking" )]
, dfrc[idx.stagedeb,c("tumorstage", "debulking","risk")]
, t0=365.25*3, 
title="All Training Datasets")
.reclassPlot(dfrc[idx.stagedeb,"y"],dfrc[idx.stagedeb,c("tumorstage",
"debulking" )]
, dfrc[idx.stagedeb,c("tumorstage", "debulking","risk")]
, t0=365.25*5, 
title="All Training Datasets")
@
\caption{The additional value of our gene signature compared to age and
debulking status alone.  These plots visualize the Integrated Discrimination
Improvement (IDI, \cite{Pencina:2008,Uno:2012}), a measure of discrimination
of two competing nested survival models. Shown are plots for events within the
first 1, 3 and 5 years, respectively. The y-axis shows the probability that
the differences in predicted risk between the new and old models are smaller
than $s$, $P(D \leq s)$. The bold curve shows this probability for patients
who suffered from an event within the specified time, the thin line for those
who did not. The new model is better when patients with events have a higher
estimated risk (low probability that the risk difference is smaller than $s$)
and those without event a lower risk (high probability that risk difference is
smaller than $s$). The y-axis shows the discrimination improvement by varying
$s$, with the grey dots representing the IDI. See also \cite{Uno:2012} for a
more detailed explanation of these plots.  Our gene signature improved for all
three tested time points a model using only debulking status and age.} 
\label{fig:reclass}
\end{figure}

\begin{figure}
<<plotmodeltcgavalidation, results='hide'>>=
par(mfrow=c(2,2))

plot(model.tcga,newdata=TCGA.validation.eset,newy=TCGA.validation.eset$y,censor.at=60,
            cex.base=1.4,show.n.risk=FALSE, show.HR=FALSE,
            show.legend=FALSE,xlab="",ylab="Overall Survival (%)",main="TCGA test set")

plot(model.tcga,newdata=GSE9891_eset,newy=GSE9891_eset$y,censor.at=60,
            cex.base=1.4,show.n.risk=FALSE, show.HR=FALSE,
            show.legend=FALSE,xlab="",ylab="",main="Tothill")
            
plot(model.tcga,newdata=GSE26712_eset,newy=GSE26712_eset$y,censor.at=60,
            cex.base=1.4,show.n.risk=FALSE, show.HR=FALSE,
            show.legend=FALSE,xlab="Time (Months)",ylab="Overall Survival (%)",main="Bonome")

plot(model.tcga,newdata=PMID17290060_eset,newy=PMID17290060_eset$y,censor.at=60,
            cex.base=1.4,show.n.risk=FALSE, show.HR=FALSE,
            show.legend=FALSE,xlab="Time (Months)",ylab="",main="Dressman")
@
\caption{TCGA model applied to the author test sets shown in Figure 2c of the TCGA paper
\cite{TCGA:2011}. The identical results show that our implementation of the
TCGA model is correct. Red survival curves correspond to high risk patients,
blue curves to low risk patients.}
\label{fig:tcga:reproduce}
\end{figure}

\begin{figure}
<<reclassification,cache=TRUE,fig.width=12,fig.height=3,out.width="0.9\\textwidth",out.height="0.3\\textwidth",results='hide'>>=
par(mfrow=c(1,3))
.reclassPlot(dfrc$y, dfrc$risk_tcga, dfrc$risk, t0=365.25, 
title="All Training Datasets")
.reclassPlot(dfrc$y, dfrc$risk_tcga, dfrc$risk, t0=365.25*3, 
title="All Training Datasets")
.reclassPlot(dfrc$y, dfrc$risk_tcga, dfrc$risk, t0=365.25*5, 
title="All Training Datasets")
@
\caption{Comparison of all \textit{leave-one-dataset-out} cross-validated risk
scores and the risk scores calculated by the TCGA model.  See
\textbf{Supplemental Figure~\ref{fig:reclass}} and \cite{Uno:2012} for an
explanation of these plots.  Shown are again plots for events within the first
1, 3 and 5 years, respectively. Our model shows a small but consistent
reclassification improvement over all three time points.}
\label{fig:reclass:tcga}
\end{figure}

% Calculate the C-Index differences
<<concdeltaclinical1, cache=TRUE>>=
res.dfrc <- Inf.Cval.Delta(cbind(as.numeric(dfrc$days_to_death[idx.stagedeb]),
dfrc$vital_status[idx.stagedeb]=="deceased"),
cbind(dfrc$tumorstage[idx.stagedeb], dfrc$debulking[idx.stagedeb] ), 
cbind(dfrc$tumorstage[idx.stagedeb],
dfrc$debulking[idx.stagedeb], dfrc$risk[idx.stagedeb]),tau=365.25*4)
@

<<concdeltaclinical2, cache=TRUE>>=
res.dfrc2 <- Inf.Cval.Delta(cbind(as.numeric(dfrc$days_to_death[idx.stagedeb]),
dfrc$vital_status[idx.stagedeb]=="deceased"),
dfrc$risk[idx.stagedeb],
cbind(dfrc$tumorstage[idx.stagedeb],
dfrc$debulking[idx.stagedeb], dfrc$risk[idx.stagedeb]),tau=365.25*4)
@
<<formatdeltac>>=
.formatDC <- function(conf) {
          conf <- sapply(conf, function(x) format(x, digits=2))
          dc <- paste(conf[1],"; 95\\% CI, ", conf[3], " - ",
                      conf[4], sep="")
}

@
\subsection{Comparison with clinicopathologic and demographic characteristics}

A model including both risk score and clinical variables was superior to a
model based on clinical factors alone. This was true both in terms of
concordance ($\Delta C$ \Sexpr{.formatDC(res.dfrc[3,])}) and reclassification
within the first 1, 3 and 5 years after diagnosis (Supplemental
Figure~\ref{fig:reclass}).  Finally, the combined model provided a modest
improvement over the gene signature alone ($\Delta C$
\Sexpr{.formatDC(res.dfrc2[3,])} in the six datasets with available clinical
factors.

<<unocdiff, cache=TRUE>>=
res.c.delta.tcga <- Inf.Cval.Delta(cbind(as.numeric(dfrcboth$days_to_death), dfrcboth$vital_status=="deceased"), dfrcboth$risk_tcga, dfrcboth$risk,tau=365.25*4)
@


\subsection{Comparison with the TCGA signature}

We then compared our signature to the TCGA signature. To apply the TCGA
signature across microarray platforms, we matched the 193 probe sets in the
signature to unique gene symbols used in curatedOvarianData \cite{Seal:2011};
this approach led to 185 unique genes involved in our signature.  We first
reproduced the reported performance of this model in the three TCGA test
datasets \cite{Bonome:2008,Dressman:2007,Tothill:2008} to ensure the
correctness of our model implementation (Supplemental
Figure~\ref{fig:tcga:reproduce}). Among the 200 genes in our signature, 19
overlapped with the TCGA signature (p < 0.001).  The probability of this
overlap between our and the TCGA signature was calculated with the
hypergeometric distribution, using the number of genes common to all training
datasets as background.  The C-Index of our signature compared to TCGA
improved moderately (\Sexpr{.formatDC(res.c.delta.tcga[3,])}). As random
signatures in ovarian cancer achieve a C-Index of 0.54 (Waldron et al.,
submitted), our signature represents a 33\% C-Index improvement over TCGA
compared to random signatures. 
\clearpage

\section{Prediction of refractory cases}

\begin{figure}
<<elmeta,results='hide'>>=
Xs.all <- lapply(esets.f, .dichotomizeshortlong, l=365.25, label="os_1yr")
preds <- lapply(1:length(Xs.all), function(i) predict(ma[[10]]$fits[[i]]$fit,
newdata=Xs.all[[i]])@lp)
Xs.all <- c(Xs.all, lapply(esets.validation_orig[3], .dichotomizeshortlong, l=365.25,
label="os_1yr"))
preds <- c(preds,lapply(length(Xs.all), function(i)
predict(final.model, newdata=Xs.all[[i]])@lp))

preds_tcga <- lapply(1:length(Xs.all), function(i) predict(model.tcga,
newdata=Xs.all[[i]])@lp)

labels <- lapply(Xs.all, function(X) X$os_1yr)
titles <- .getDatasetNames(Xs.all)
# remove TCGA again because we compare to the TCGA model
idx <- -8

aucs <- .plotROCpanel(preds[idx],labels[idx],titles[idx],3,3)
aucs_tcga <-
.plotROCpanel(preds_tcga[idx],labels[idx],titles[idx],2,3,plot=FALSE)
@
\caption{Prediction of refactory cases in a \textit{leave-one-dataset-out}
cross-validation.  Here the patient risk scores obtained by our overall
survival signature were used to predict events in the first year. ROC curves
visualize true and false positive rates for all possible risk score cutoffs.
The higher the score, the higher the probability the patient will die within
the first year. For each dataset, the overall survival signature was trained
using only the remaining datasets (Figure 1).}
\label{fig:roc:1yr}
\end{figure}

We evaluated the ability of our overall survival signature to predict
deaths within the first 12 months after diagnosis of the disease. This
endpoint was used as a surrogate for the presence of tumors refractory to
first-line adjuvant chemotherapy. In Supplemental Figure~\ref{fig:roc:1yr}, we
show ROC curves for our meta-analysis signature alone. The pooled AUC was
\Sexpr{.formatDC(aucs[c(1,2,5,6)])}, compared to 
\Sexpr{.formatDC(aucs_tcga[c(1,2,5,6)])} for the TCGA signature. 

<<oneyrfun, results='asis'>>=
df1yr <- .createClinical(Xs.all[idx], c("age_at_initial_pathologic_diagnosis",
"debulking","os_1yr", "tumorstage", "debulking", "days_to_death",
"vital_status"))
df1yr$os_1yr <- as.factor(df1yr$os_1yr)

df1yr$risk <- unlist(preds_tcga[idx])
fit_tcga <-
    glm(os_1yr~risk+age_at_initial_pathologic_diagnosis+debulking,data=df1yr,family="binomial")

df1yr$risk <- unlist(preds[idx])
fit <-
    glm(os_1yr~risk+age_at_initial_pathologic_diagnosis+debulking,data=df1yr,family="binomial")

fit_clinical <-
    glm(os_1yr~age_at_initial_pathologic_diagnosis+debulking,data=df1yr,family="binomial")

outreg(list(fit,fit_tcga), modelLabels=c("Meta-Analysis", "TCGA"),
title="Prediction of refractory cases. The table lists two logistic
regressions, one for our leave-one-dataset-out cross-validated meta-analysis gene signature, one for the TCGA
signature. The predictions from both models were adjusted for age and debulking
status. ", label="tbl:reg:1yr", tight=FALSE,
varLabels=list(risk="Gene Signature",debulkingsuboptimal="Debulking (Suboptimal)", 
age_at_initial_pathologic_diagnosis="Age"))

set.seed(1234)
aucs.adj <- repCV(fit, cost=ci.auc,R=10)
aucs_clinical.adj <- repCV(fit_clinical, cost=ci.auc,R=10)
@

We then adjusted our gene signature risk score for age and debulking status
(Supplemental Table~\ref{ tbl:reg:1yr }). While significant, both our and the
TCGA signature provided only modest improvement over clinical factors in
predicting early events. In a five-fold cross-validation, the AUC 
of the 
multivariate model was \Sexpr{.formatDC(aucs.adj[[4]][c(2,1,1,3)])} compared to 
\Sexpr{.formatDC(aucs_clinical.adj[[4]][c(2,1,1,3)])} for a 
model utilizing only age and debulking status.

% add the TCGA early stage samples to the validation data
<<addearlystage>>=
ids <- esets.allos$TCGA_eset$summarystage=="early" &
    esets.allos$TCGA_eset$summarygrade!="low"
esets.validation <- c(esets.validation_orig, TCGA_eset=esets.allos$TCGA_eset[, ids])
@

% make another data.frame with clinical information as signature risk scores,
% this time for the validation data
<<clinicalvalidation, cache=TRUE>>=
X <- esets.validation
dfrcval <- .createClinical(esets.validation)
for (i in 1:length(X)) X[[i]]$risk <- predict(final.model, newdata=X[[i]], type="lp")@lp
dfrcval$risk <- unlist(sapply(X, function(x) x$risk))
for (i in 1:length(X)) X[[i]]$risk <- predict(model.tcga, newdata=X[[i]], type="lp")@lp
dfrcval$risk_tcga <- unlist(sapply(X, function(x) x$risk))
@

% Prepare the debulking datasets (use only genes common to all platforms and
% use only stage III patients)
<<debulkingmetaprepare>>=
Xs <- esets.debulking 
Xs.all <- metaCMA.common.gene.esets(Xs)
for (i in 1:length(Xs.all)) Xs.all[[i]]$debulking <- as.factor(Xs.all[[i]]$debulking)
Xs.alls3 <- lapply(Xs.all, function(X) X[,X$tumorstage==3 |
is.na(X$tumorstage)])
@

<<debulkingmetas3, cache=TRUE, results='hide'>>=
# use only stage 3 patients for training, as stage 4 patients typically don't
# get debulking surgery
coefs.debulking.s3 <- metaCMA.coefs(Xs.alls3, y="debulking", family=binomial)
res.debulking.s3  <- metaCMA(Xs.alls3,y="debulking",coefs=coefs.debulking.s3, n=200,
filter.fun=.debulkingFilter)
@
\section{Prediction of suboptimal debulking surgery}
We finally generated a gene signature for suboptimal debulking surgery and
validated this signature by \textit{leave-one-dataset-out} cross-validation.
In addition to the meta-analysis summary in Figure 5 of the main paper, we
show here in Supplemental Figure~\ref{fig:roc:debulking} the corresponding ROC
curves.

\begin{figure}
<<debulkingmetaplot, cache=TRUE>>=
preds <- lapply(res.debulking.s3$fits,function(fit) fit$risk@lp)
labels <- lapply(Xs.alls3, function(X) X$debulking)
titles <- .getDatasetNames(Xs.alls3)
idx <- !sapply(Xs.alls3,.debulkingFilter)
preds.logit <- .p2logit(Xs.alls3[idx], fits=res.debulking.s3$fits[idx])
aucs.db <- .plotROCpanel(preds.logit,labels[idx],titles[idx],3,3)
@
\caption{Prediction of suboptimally debulked tumors in a leave-one-dataset-out
cross-validation. The prediction model calculates for each sample a score. The
higher the score, the higher the probability the tumor will be not optimally
debulkable. For each dataset, the model is trained using only the remaining
datasets. The model utilizes gene expression only (\textbf{Supplemental Table
2}). ROC curves visualize the true and false positive rates as a function of
the probability cutoffs.}  
\label{fig:roc:debulking}
\end{figure}

% Write the final debulking model to a spreadsheet.
<<finalmodeldebulking,cache=TRUE>>=
tmp <- metaCMA.opt(Xs.alls3, coefs=coefs.debulking.s3,n=200,y="debulking")
final.model.debulking <- tmp$model
sig <- names(final.model.debulking@coefficients)

probesets <- do.call(cbind, lapply(Xs.all, function(X)
 as.character(featureData(X)[sig,]$maxmean_probeset)))

write.csv(data.frame(Genes=sig, probesets, 
"Pooled LogIt Coefficient"=
final.model.debulking@coefficients,
"Mean Optimal"= apply(tmp$train$X[which(tmp$train$y=="optimal"),],2,mean),
"Mean Suboptimal"= apply(tmp$train$X[which(tmp$train$y=="suboptimal"),],2,mean),
 P=head(sort(tmp$pvalues),length(sig)),
 FDR=head(sort(p.adjust(tmp$pvalues,method="BH")),length(sig))),
 file="final_signature_debulking.csv",row.names=FALSE)
@
<<writemodelobjs>>=
save(final.model, final.model.debulking, file="models.rda")
@

<<finalmodeldebulkingfoldchanges, cache=TRUE>>=
load("input/eset.debulking.rda")
esets.db <- esets
esets.db <- esets.db[!sapply(esets.db, .debulkingFilter)]

res <- metaCMA.limma(lapply(esets.db, function(X)
    X[names(final.model.debulking@coefficients),]), "debulking",
    "suboptimal-optimal")
resM <- do.call(cbind, lapply(res, function(r) topTable(r,number=200,
    sort.by="none")[,2]))
resM <- cbind(resM, Weighted.Mean=apply(resM,1, weighted.mean, w=sqrt(sapply(
    esets.db, ncol))))
rownames(resM) <- names(final.model.debulking@coefficients)
resM <- cbind(resM,
FDR=head(sort(p.adjust(tmp$pvalues,method="BH")),length(sig)))
resM <- resM[order(abs(resM[,ncol(resM)-1]),decreasing=TRUE),]
resM[,1:9] <- 2^resM[,1:9]
resM[,1:9] <- apply(resM[,1:9],c(1,2),function(x) ifelse(x<1, 1/(x*-1),x))


write.csv(resM, "final_signature_debulking_foldchanges.csv")
@

\clearpage
\bibliographystyle{plain}
\bibliography{metasig}
\newpage
\clearpage
\appendix
\section{Main Figures}
This supplement was generated with the knitr R package and contains the
complete analysis of this study. The source code is available at
\url{https://bitbucket.org/lima1/ovrc4_signew} to ease the reproducibility of
the study.  The following section generates the figures of the main paper.
\renewcommand{\thefigure}{\arabic{figure}}
\setcounter{figure}{0}

\begin{figure}
<<kmplotstrain,results='hide'>>=
res.lodo <- .lodocvPlot(esets.f, models=lapply(ma[[10]]$fits, function(x)
x$fit),ids=1:7)
res.lodo.tcga <- .lodocvPlot(esets.f, plot=FALSE,models=model.tcga,ids=1:7)
@
\caption{\textit{Leave-one-dataset-out} cross-validation of a novel gene signature for
predicting overall survival in late stage ovarian cancer.
See caption in the main paper for details.}
\end{figure}

<<datasetshr2prepare>>=
.allCombsEval <- function(ids, esets, object) {
       lodo <- .lodocvPlot(esets, models=object$fits[[1]]$fit, ids=ids,
       plot=FALSE)[[1]]
       fit <- coxph(lodo$y~lodo$strata)
       summary(fit)$coefficients
}
@

<<datasetshr2, cache=TRUE>>=
ret.hr <- lapply(1:length(esets.f), metaCMA.allcombinations, esets.f, n=200,eval.fun=.allCombsEval )
@

<<plotdatasetshrprepare>>=

df.hr <- do.call(rbind, lapply(1:length(esets.f),
function(i) data.frame(.getDatasetNames(esets.f)[i], unlist(ret.hr[[i]]$n),
  sapply(ret.hr[[i]]$evaluation, function(y) 1/y[2]))))
 
colnames(df.hr) <-c("Dataset", "n", "HR")

.psContains <- function(i, esets, s) {
    .doPS <- function(ps) {
        idx <- (1:length(esets))[-i][ps]
        ifelse(sum(grepl(s, names(esets)[idx]))>0, "Yes", "No")
     }
     pss <- metaCMA.powerset(length(esets[-i]))
     ret <- lapply(pss, .doPS)
}

df.hr$containstcga <- unlist(lapply(1:length(esets.f), .psContains, esets.f,"TCGA"))

df.hr <- df.hr[df.hr[,1]!="TCGA 2011",]

p <-ggplot(df.hr, aes(n,HR))+geom_smooth(method="lm")
#p <- p +geom_point(aes(shape=containstcga))
p <- p +geom_point(size=0.75)
p <- p +facet_wrap(~Dataset)+coord_cartesian(ylim= c(0.75, 3))+ylab("Hazard Ratio")+xlab("Total Training Sample Size")
#p <- p+ scale_shape(solid=FALSE, name = "TCGA used for training")
p <- p +theme(axis.text.x=element_text(angle=45,hjust=1))+theme_grey(base_size=14)
@

\begin{figure}
<<plotdatasetshr, cache=TRUE>>=
plot(p) 
@
\caption{Hazard Ratios as a function of training sample size.
See caption in the main paper for details.}
\end{figure}


\begin{figure}
<<validationexternal,cache=TRUE,fig.width=7,fig.height=4.7,out.width="0.9\\textwidth",out.height="0.6\\textwidth",results='hide'>>=
res.val <- .valPlot(final.model)
res.val.tcga <- .valPlot(model.tcga, plot=FALSE)
@
\caption{Validation of the final signature in independent data. 
See caption in the main paper for details.}
\end{figure}

\begin{figure}
\subfigure[]{
<<comptcga,fig.width=7,fig.height=7,out.width="0.6\\textwidth",out.height="0.6\\textwidth",results='hide'>>=
dfrcval$debulking_orig <- dfrcval$debulking
dfrcboth <- rbind(dfrc, dfrcval)
dfrcboth <- dfrcboth[- grep("TCGA_eset", rownames(dfrcboth)),]

# thanks Hmisc for messing up the Surv function
dfrcboth$y <- Surv(dfrcboth$y[,1], dfrcboth$y[,2])


# get dichotomized risk scores, not from the early stage samples
strata_we <-   do.call(rbind, lapply(c(res.lodo,res.val[-4]), function(x)
data.frame(x$strata, x$y)))
strata_tcga <-   do.call(rbind, lapply(c(res.lodo.tcga,res.val.tcga[-4]), function(x)
data.frame(x$strata, x$y)))


par(mfrow=c(2,2))
plotKM(y=Surv(strata_we[,2][,1],strata_we[,2][,2]),
strata=strata_we[,1],censor.at=365.25*5,
        show.n.risk=FALSE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.1,
        show.PV=FALSE,
        xlab="Time (Days)",ylab="Overall Survival (%)",
        cex.base=1.4,main="A)   Meta-Analysis Signature")
plotKM(y=Surv(strata_tcga[,2][,1],strata_tcga[,2][,2]),
strata=strata_tcga[,1],censor.at=365.25*5,
        show.n.risk=FALSE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.1,
        show.PV=FALSE,
        xlab="Time (Days)",ylab="Overall Survival (%)",
        cex.base=1.4,main="B)   TCGA Signature")
plotKM(y=dfrcboth$y,strata=factor(dfrcboth$debulking_orig,levels=c("suboptimal", "optimal")),censor.at=365.25*5,
        show.n.risk=FALSE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.1,
        show.PV=FALSE,
        xlab="Time (Days)",ylab="Overall Survival (%)",
        cex.base=1.4,main="C)   Debulking (subopt. vs opt.)")
plotKM(y=dfrcboth$y, strata=factor(dfrcboth$tumorstage,levels=c(4,3)),censor.at=365.25*5,
        show.n.risk=FALSE,show.legend=FALSE,show.HR=TRUE,cex.HR=1.1,
        show.PV=FALSE,
        xlab="Time (Days)",ylab="Overall Survival (%)",
        cex.base=1.4,main="D)   Stage (4 vs. 3)")
@
}
\subfigure[]{
<<fpcomptcga,fig.width=5.75,fig.height=9,out.width="0.4\\textwidth",out.height="0.6\\textwidth",results='hide'>>=
Xs.all <- c(esets.f[-8], esets.validation[-4])
names(Xs.all) <- .getDatasetNames(Xs.all)

res.strata <- lapply(c(res.lodo, res.val[-4]), function(x) x$strata)
res.strata.tcga <- lapply(c(res.lodo.tcga, res.val.tcga[-4]), function(x) x$strata)

source("src/forestplot.surv.R")

x <- metaCMA.forest.models(metaCMA.censor(Xs.all,censor.at=365.25*5), risks1=res.strata, risks2=res.strata.tcga,
 graphwidth=unit(2.5,
 "inches"),concordance=FALSE,inverse=TRUE,x.ticks=c(0.5,1,2,4,8),clip=log(c(0.5,8)))
mtext("E)",cex=1.4,font=2)
@
}
\caption{Comparison of our novel meta-analysis gene signature with existing
prognostic factors.
See caption in the main paper for details.}
\end{figure}

<<hrdiffconfint, cache=TRUE>>=
d.f <- data.frame( do.call(rbind,lapply(metaCMA.censor(Xs.all,censor.at=365.25*5), function(X) X$y)),
strata=strata_we[,1], strata_tcga=strata_tcga[,1])

hrdiffemp <- .boostrapHRs(d.f,n=10000,inverse=TRUE)
hrdiffci <- quantile(hrdiffemp,c(0.025,0.975))
@

\begin{figure}
<<plotfpdebulking,fig.width=8,fig.height=6,out.width="0.9\\textwidth",out.height="0.675\\textwidth",results='hide'>>=
priors <- sapply(labels, function(l) sum(l=="suboptimal")/length(l))
 xxx <- .forestPlotDebulking(preds1=preds.logit,
  labels=labels[idx],
 prior=priors[idx],titles=titles[idx])
@
\caption{Predicting suboptimally debulked tumors. See caption in the main
paper for details.}
\end{figure}

\clearpage
\section{Session Info}
<<sessioninfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\end{document}
